{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition - House Prices Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar= False, ax=ax1,cmap = 'viridis')\n",
    "sns.heatmap(df_test.isnull(), yticklabels=False, cbar= False, ax=ax2,cmap = 'viridis')\n",
    "ax1.title.set_text('train')\n",
    "ax2.title.set_text('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Train dataset\n",
      "\n",
      "LotFrontage      259\n",
      "Alley           1369\n",
      "MasVnrType         8\n",
      "MasVnrArea         8\n",
      "BsmtQual          37\n",
      "BsmtCond          37\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtFinType2      38\n",
      "Electrical         1\n",
      "FireplaceQu      690\n",
      "GarageType        81\n",
      "GarageYrBlt       81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "PoolQC          1453\n",
      "Fence           1179\n",
      "MiscFeature     1406\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Test dataset\n",
      "\n",
      "MSZoning           4\n",
      "LotFrontage      227\n",
      "Alley           1352\n",
      "Utilities          2\n",
      "Exterior1st        1\n",
      "Exterior2nd        1\n",
      "MasVnrType        16\n",
      "MasVnrArea        15\n",
      "BsmtQual          44\n",
      "BsmtCond          45\n",
      "BsmtExposure      44\n",
      "BsmtFinType1      42\n",
      "BsmtFinSF1         1\n",
      "BsmtFinType2      42\n",
      "BsmtFinSF2         1\n",
      "BsmtUnfSF          1\n",
      "TotalBsmtSF        1\n",
      "BsmtFullBath       2\n",
      "BsmtHalfBath       2\n",
      "KitchenQual        1\n",
      "Functional         2\n",
      "FireplaceQu      730\n",
      "GarageType        76\n",
      "GarageYrBlt       78\n",
      "GarageFinish      78\n",
      "GarageCars         1\n",
      "GarageArea         1\n",
      "GarageQual        78\n",
      "GarageCond        78\n",
      "PoolQC          1456\n",
      "Fence           1169\n",
      "MiscFeature     1408\n",
      "SaleType           1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print ('Missing values in Train dataset\\n')\n",
    "print(df.isnull().sum()[df.isnull().sum()>0])\n",
    "print ( '\\nMissing values in Test dataset\\n')\n",
    "print(df_test.isnull().sum()[df_test.isnull().sum()>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first feature 'LotFrontage'\n",
    "\n",
    "# in train DS\n",
    "df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())\n",
    "\n",
    "# in test DS\n",
    "df_test['LotFrontage'] = df_test['LotFrontage'].fillna(df_test['LotFrontage'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['MSZoning'] = df_test['MSZoning'].fillna(df_test['MSZoning'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be dropping feature which has more than 50% missing value\n",
    "\n",
    "df.drop(columns=['Alley','PoolQC','Fence','MiscFeature'], inplace = True)\n",
    "\n",
    "df_test.drop(columns=['Alley','PoolQC','Fence','MiscFeature'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating null value with mode value\n",
    "df['FireplaceQu'] = df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\n",
    "\n",
    "df_test['FireplaceQu'] = df_test['FireplaceQu'].fillna(df_test['FireplaceQu'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['MasVnrType'] = df_test['MasVnrType'].fillna(df_test['MasVnrType'].mode()[0])\n",
    "df['MasVnrType'] = df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['MasVnrType'] = df_test['MasVnrType'].fillna(df_test['MasVnrType'].mode()[0])\n",
    "df['MasVnrType'] = df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['MasVnrArea'] = df_test['MasVnrArea'].fillna(df_test['MasVnrArea'].median())\n",
    "df['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test['BsmtQual'] = df_test['BsmtQual'].fillna(df_test['BsmtQual'].mode()[0])\n",
    "df['BsmtQual'] = df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test['BsmtCond'] = df_test['BsmtCond'].fillna(df_test['BsmtCond'].mode()[0])\n",
    "df['BsmtCond'] = df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_test['BsmtExposure'] = df_test['BsmtExposure'].fillna(df_test['BsmtExposure'].mode()[0])\n",
    "df['BsmtExposure'] = df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['BsmtFinType1'] = df_test['BsmtFinType1'].fillna(df_test['BsmtFinType1'].mode()[0])\n",
    "df['BsmtFinType1'] = df['BsmtFinType1'].fillna(df['BsmtFinType1'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test['BsmtFinType2'] = df_test['BsmtFinType2'].fillna(df_test['BsmtFinType2'].mode()[0])\n",
    "df['BsmtFinType2'] = df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test['GarageType'] = df_test['GarageType'].fillna(df_test['GarageType'].mode()[0])\n",
    "df['GarageType'] = df['GarageType'].fillna(df['GarageType'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['GarageYrBlt'] = df_test['GarageYrBlt'].fillna(df_test['GarageYrBlt'].median())\n",
    "df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['GarageYrBlt'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['GarageFinish'] = df_test['GarageFinish'].fillna(df_test['GarageFinish'].mode()[0])\n",
    "df['GarageFinish'] = df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['GarageQual'] = df_test['GarageQual'].fillna(df_test['GarageQual'].mode()[0])\n",
    "df['GarageQual'] = df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['GarageCond'] = df_test['GarageCond'].fillna(df_test['GarageCond'].mode()[0])\n",
    "df['GarageCond'] = df['GarageCond'].fillna(df['GarageCond'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Utilities'] = df_test['Utilities'].fillna(df_test['Utilities'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Exterior1st'] = df_test['Exterior1st'].fillna(df_test['Exterior1st'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Exterior2nd'] = df_test['Exterior2nd'].fillna(df_test['Exterior2nd'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['BsmtFinSF2'] = df_test['BsmtFinSF2'].fillna(df_test['BsmtFinSF2'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['BsmtUnfSF'] = df_test['BsmtUnfSF'].fillna(df_test['BsmtUnfSF'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['TotalBsmtSF'] = df_test['TotalBsmtSF'].fillna(df_test['TotalBsmtSF'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['BsmtFullBath'] = df_test['BsmtFullBath'].fillna(df_test['BsmtFullBath'].median())\n",
    "\n",
    "df_test['BsmtHalfBath'] = df_test['BsmtHalfBath'].fillna(df_test['BsmtHalfBath'].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['KitchenQual'] = df_test['KitchenQual'].fillna(df_test['KitchenQual'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "df_test['BsmtFinSF1'] = df_test['BsmtFinSF1'].fillna(df_test['BsmtFinSF1'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test['Functional'] = df_test['Functional'].fillna(df_test['Functional'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['GarageCars'] = df_test['GarageCars'].fillna(df_test['GarageCars'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2138a256888>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test.GarageArea.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['GarageArea'] = df_test['GarageArea'].fillna(df_test['GarageArea'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['SaleType'] = df_test['SaleType'].fillna(df_test['SaleType'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID column is also repeatitive, so can be removed from dataframe\n",
    "\n",
    "df.drop(columns=['Id'], inplace=True)\n",
    "df_test.drop(columns=['Id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar= False, ax=ax1,cmap = 'viridis')\n",
    "sns.heatmap(df_test.isnull(), yticklabels=False, cbar= False, ax=ax2,cmap = 'viridis')\n",
    "ax1.title.set_text('train')\n",
    "ax2.title.set_text('test')\n",
    "\n",
    "## no null value exist now in both the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 76), (1459, 75))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64'), dtype('O'), dtype('float64')], dtype=object)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first find all categorical columns \n",
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = df.dtypes[df.dtypes=='O'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df.copy()\n",
    "df_test_main = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 76), (1459, 75))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.shape, df_test_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns == df.columns[:-1]\n",
    "\n",
    "# all the columns are alligned properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding one column in the end at test df\n",
    "df_test[df.columns[-1]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df, df_test], axis = 0)\n",
    "df_final.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 76)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have lot of categorical features, will make a function to \n",
    "# handle all categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_to_dummies(cat_col,df_final):\n",
    "\n",
    "    # This function helps in converting categorical variables into onehot coding\n",
    "    # cat_col : it is a list/array of the all categorical variables in dataframe\n",
    "    # df_final : it is the dataframe\n",
    "    \n",
    "    for col in cat_col:\n",
    "        #print(col)\n",
    "        df_dummy = pd.get_dummies(df_final[col],drop_first=True,prefix=col)\n",
    "        df_final.drop(col, axis = 1, inplace = True)\n",
    "        \n",
    "        df_final = pd.concat([df_final,df_dummy],axis=1)\n",
    "        \n",
    "    return df_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=categories_to_dummies(cat_col=cat_col,df_final=df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 237)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent variable in the end\n",
    "\n",
    "col_list = list(df_final.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding index of a string in a list\n",
    "col_list.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to add it in the end\n",
    "col_list.append('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final= df_final[col_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>MSZoning_FV</th>\n",
       "      <th>MSZoning_RH</th>\n",
       "      <th>MSZoning_RL</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "      <th>Street_Pave</th>\n",
       "      <th>LotShape_IR2</th>\n",
       "      <th>LotShape_IR3</th>\n",
       "      <th>LotShape_Reg</th>\n",
       "      <th>LandContour_HLS</th>\n",
       "      <th>LandContour_Low</th>\n",
       "      <th>LandContour_Lvl</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "      <th>LotConfig_CulDSac</th>\n",
       "      <th>LotConfig_FR2</th>\n",
       "      <th>LotConfig_FR3</th>\n",
       "      <th>LotConfig_Inside</th>\n",
       "      <th>LandSlope_Mod</th>\n",
       "      <th>LandSlope_Sev</th>\n",
       "      <th>Neighborhood_Blueste</th>\n",
       "      <th>Neighborhood_BrDale</th>\n",
       "      <th>Neighborhood_BrkSide</th>\n",
       "      <th>Neighborhood_ClearCr</th>\n",
       "      <th>Neighborhood_CollgCr</th>\n",
       "      <th>Neighborhood_Crawfor</th>\n",
       "      <th>...</th>\n",
       "      <th>BsmtFinType2_Rec</th>\n",
       "      <th>BsmtFinType2_Unf</th>\n",
       "      <th>Heating_GasA</th>\n",
       "      <th>Heating_GasW</th>\n",
       "      <th>Heating_Grav</th>\n",
       "      <th>Heating_OthW</th>\n",
       "      <th>Heating_Wall</th>\n",
       "      <th>HeatingQC_Fa</th>\n",
       "      <th>HeatingQC_Gd</th>\n",
       "      <th>HeatingQC_Po</th>\n",
       "      <th>HeatingQC_TA</th>\n",
       "      <th>CentralAir_Y</th>\n",
       "      <th>Electrical_FuseF</th>\n",
       "      <th>Electrical_FuseP</th>\n",
       "      <th>Electrical_Mix</th>\n",
       "      <th>Electrical_SBrkr</th>\n",
       "      <th>KitchenQual_Fa</th>\n",
       "      <th>KitchenQual_Gd</th>\n",
       "      <th>KitchenQual_TA</th>\n",
       "      <th>Functional_Maj2</th>\n",
       "      <th>Functional_Min1</th>\n",
       "      <th>Functional_Min2</th>\n",
       "      <th>Functional_Mod</th>\n",
       "      <th>Functional_Sev</th>\n",
       "      <th>Functional_Typ</th>\n",
       "      <th>FireplaceQu_Fa</th>\n",
       "      <th>FireplaceQu_Gd</th>\n",
       "      <th>FireplaceQu_Po</th>\n",
       "      <th>FireplaceQu_TA</th>\n",
       "      <th>GarageType_Attchd</th>\n",
       "      <th>GarageType_Basment</th>\n",
       "      <th>GarageType_BuiltIn</th>\n",
       "      <th>GarageType_CarPort</th>\n",
       "      <th>GarageType_Detchd</th>\n",
       "      <th>GarageFinish_RFn</th>\n",
       "      <th>GarageFinish_Unf</th>\n",
       "      <th>GarageQual_Fa</th>\n",
       "      <th>GarageQual_Gd</th>\n",
       "      <th>GarageQual_Po</th>\n",
       "      <th>GarageQual_TA</th>\n",
       "      <th>GarageCond_Fa</th>\n",
       "      <th>GarageCond_Gd</th>\n",
       "      <th>GarageCond_Po</th>\n",
       "      <th>GarageCond_TA</th>\n",
       "      <th>PavedDrive_P</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_ConLI</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>856.0</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>208500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1262.0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>181500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>223500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea    ...      SaleCondition_Normal  SaleCondition_Partial  SalePrice\n",
       "0          60         65.0     8450    ...                         1                      0   208500.0\n",
       "1          20         80.0     9600    ...                         1                      0   181500.0\n",
       "2          60         68.0    11250    ...                         1                      0   223500.0\n",
       "3          70         60.0     9550    ...                         0                      0   140000.0\n",
       "4          60         84.0    14260    ...                         1                      0   250000.0\n",
       "\n",
       "[5 rows x 237 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_final[:1460]\n",
    "df_test = df_final[1460:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes[df_train.dtypes=='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop(['SalePrice'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1459, 236), (1460, 237))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['SalePrice'], axis=1)\n",
    "y = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing the zero variance features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_thre.get_support()\n",
    "# all false are having zero variance and respective feature would be removed\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thre = VarianceThreshold(threshold=0)\n",
    "var_thre.fit(X_train)\n",
    "\n",
    "\n",
    "rem_feature = X_train.columns.values[~var_thre.get_support()]\n",
    "\n",
    "X_train.drop(columns=rem_feature.tolist(),inplace=True)\n",
    "X_test.drop(columns=rem_feature.tolist(), inplace=True)\n",
    "df_test.drop(columns=rem_feature.tolist(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression data - Checking relation between all the independent variables with dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for regression problem, this helps in finding out the most correlated feature with respect to target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_columns(df, thresh):\n",
    "    col_corr = set()\n",
    "    corr = df.corr()\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i):\n",
    "            if abs(corr.iloc[i,j]) > thresh:\n",
    "                col_corr.add(corr.columns[i])\n",
    "    return col_corr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_feature= list(correlated_columns(X_train, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=rem_feature,inplace=True)\n",
    "X_test.drop(columns=rem_feature, inplace=True)\n",
    "df_test.drop(columns=rem_feature, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168, 211)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance - ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was not applied ( below is for classification problem)\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "extra = ExtraTreesClassifier(n_estimators = 5, criterion ='gini', max_features = 2)\n",
    "extra.fit(X_train, y_train) \n",
    "feature_importance = extra.feature_importances_ \n",
    "\n",
    "feature_imp_mean = np.mean([tree.feature_importances_ for tree in extra.estimators_], axis = 0) \n",
    "\n",
    "score = pd.DataFrame({'Col_name':X_train.columns, 'Score':feature_imp_mean})\n",
    "score.sort_values(by = 'Score', ascending=False).head()\n",
    "\n",
    "# removing with least score from tail ( picking 50 from down)\n",
    "\n",
    "remove_col = score.sort_values(by = 'Score', ascending=False).iloc[-100:, 0]\n",
    "X_train.drop(columns=remove_col.tolist(),inplace=True)\n",
    "X_test.drop(columns=remove_col.tolist(), inplace=True)\n",
    "df_test.drop(columns=remove_col.tolist(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest - selecting most corelated features via chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was not applied ( below is for classification problem)\n",
    "# another way to find out the relation between variables\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_regression,VarianceThreshold,mutual_info_regression\n",
    "\n",
    "bestfeature = SelectKBest(score_func=chi2, k=50)\n",
    "bestF = bestfeature.fit(X_train, y_train)\n",
    "\n",
    "dfscore = pd.DataFrame(bestF.scores_, columns=['score'])\n",
    "dfscore['FeatureName'] = X_train.columns\n",
    "dfscore['score'] = dfscore.score.round(decimals=2)\n",
    "dfscore.sort_values(ascending=False, by = 'score').head()\n",
    "\n",
    "# picking only top 100 features which relates with dependent variable\n",
    "remove_col = dfscore.sort_values(ascending=False, by = 'score').iloc[100:,1]\n",
    "X_train.drop(columns=remove_col.tolist(),inplace=True)\n",
    "X_test.drop(columns=remove_col.tolist(), inplace=True)\n",
    "df_test.drop(columns=remove_col.tolist(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1168, 100), (292, 100), (1459, 100), (1168,), (292,))"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, df_test.shape,y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = MinMaxScaler()\n",
    "sc_y = MinMaxScaler()\n",
    "\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.transform(X_test)\n",
    "df_test = sc_x.transform(df_test)\n",
    "\n",
    "y_train = sc_y.fit_transform(y_train.values.reshape(-1,1))\n",
    "y_test = sc_y.transform(y_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try for multiple model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the model                R2 score                        RMSE(log)            Time taken\n",
      " XGB                            0.9049836220753493             0.3524144120316477        1              \n",
      " ADAboost                       0.834152956944717              0.4604952975605901        1              \n",
      " DecisionTree                   0.8014204001031725             0.3946119372251829        0              \n",
      " RandomForest                   0.8984231536477993             0.35926083433175804       4              \n",
      "And the best model by r2score is : XGB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from  datetime import datetime\n",
    "start_time = int(datetime.now().strftime('%H%M%S'))\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(('XGB',XGBRegressor()))\n",
    "models.append(('ADAboost',AdaBoostRegressor()))\n",
    "models.append(('DecisionTree', DecisionTreeRegressor()))\n",
    "models.append(('RandomForest', RandomForestRegressor()))\n",
    "#models.append(('LinearRegression', LinearRegression()))\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "best_model = ''\n",
    "r2 = 0 \n",
    "\n",
    "print(f'Name of the model                R2 score                        RMSE(log)            Time taken')\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    res = r2_score(y_test, y_pred)\n",
    "    \n",
    "    if res > r2:\n",
    "        best_model = name\n",
    "        r2 = res\n",
    "        \n",
    "    \n",
    "    mse = np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred)))\n",
    "    results.append((name,res,mse))\n",
    "    \n",
    "    \n",
    "    current_time = int(datetime.now().strftime('%H%M%S'))\n",
    "    time_taken = current_time - start_time\n",
    "    start_time = current_time \n",
    "    \n",
    "    print( f' {name:<30} {res:<30} {mse:<25} {time_taken:<15}' )\n",
    "    \n",
    "\n",
    "print( f'And the best model by r2score is : { best_model}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8981710808907131"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first predicting with Random forest\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = sc_y.inverse_transform(df_pred.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing for sample submission\n",
    "\n",
    "df_sample = pd.read_csv('sample_submission_init.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 2)"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['SalePrice_pred'] =df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=df_sample[['Id','SalePrice_pred']].rename(columns = {'SalePrice_pred':'SalePrice'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv('sample_submission.csv', index= False) # this will overwrite the old file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>128621.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>154510.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>186042.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>186566.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>200971.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id  SalePrice\n",
       "0  1461  128621.00\n",
       "1  1462  154510.00\n",
       "2  1463  186042.87\n",
       "3  1464  186566.00\n",
       "4  1465  200971.83"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('sample_submission.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1506\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9004849455094691"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first predicting with XGB\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = sc_y.inverse_transform(df_pred.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('sample_submission_init.csv')\n",
    "df_sample['SalePrice_pred'] = df_pred\n",
    "df_sample=df_sample[['Id','SalePrice_pred']].rename(columns = {'SalePrice_pred':'SalePrice'})\n",
    "df_sample.to_csv('sample_submission.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score is 0.14800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try with randomizedsearchCV\n",
    "\n",
    "n_estimators = [100,300,600,900,1200,1500]\n",
    "max_depth = [2,6,9,12,15]\n",
    "booster = ['gbtree', 'gblinear']\n",
    "learning_rate = [0.02,0.04,0.08,0.1,0.15,0.2]\n",
    "min_child_weight = [1,2,3,4,5,6]\n",
    "base_score = [0.2,0.4,0.6,0.8,1]\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators' : n_estimators,\n",
    "    'max_depth' : max_depth,\n",
    "    'learning_rate' : learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster': booster,\n",
    "    'base_score': base_score   \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cv = RandomizedSearchCV(estimator=model,param_distributions=hyperparameter_grid,\n",
    "                              cv= 10, n_iter = 50,\n",
    "                              scoring = 'neg_root_mean_squared_error',\n",
    "                              n_jobs = 4,\n",
    "                              verbose = 5,\n",
    "                              return_train_score = True,\n",
    "                              random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None, gamma=None,\n",
       "                                          gpu_id=None, importance_type='gain',\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=None,\n",
       "                                          n_estimators=100,...\n",
       "                                          verbosity=None),\n",
       "                   n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.2, 0.4, 0.6, 0.8, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.02, 0.04, 0.08, 0.1,\n",
       "                                                          0.15, 0.2],\n",
       "                                        'max_depth': [2, 6, 9, 12, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5, 6],\n",
       "                                        'n_estimators': [100, 300, 600, 900,\n",
       "                                                         1200, 1500]},\n",
       "                   random_state=42, return_train_score=True,\n",
       "                   scoring='neg_root_mean_squared_error', verbose=5)"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.4, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "             min_child_weight=6, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=1200, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating the XGB with best estimators from randomsearchcv\n",
    "\n",
    "model = XGBRegressor(base_score=0.4, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "             importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
    "             min_child_weight=6, missing=np.nan, monotone_constraints='()',\n",
    "             n_estimators=1200, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9116819013341457"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = sc_y.inverse_transform(df_pred.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('sample_submission_init.csv')\n",
    "df_sample['SalePrice_pred'] = df_pred\n",
    "df_sample=df_sample[['Id','SalePrice_pred']].rename(columns = {'SalePrice_pred':'SalePrice'})\n",
    "df_sample.to_csv('sample_submission.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score is 0.13858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, PReLU, ELU, Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1168, 111), (292, 111), (1168, 1), (292, 1))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= np.append(X_train,X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.append(y_train, y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping( monitor='val_loss',verbose=1, patience=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0565 - mse: 0.0565 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 2/1000\n",
      "117/117 [==============================] - 0s 778us/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.0098 - val_mse: 0.0098\n",
      "Epoch 3/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0080 - val_mse: 0.0080\n",
      "Epoch 4/1000\n",
      "117/117 [==============================] - 0s 776us/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 5/1000\n",
      "117/117 [==============================] - 0s 720us/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0059 - val_mse: 0.0059\n",
      "Epoch 6/1000\n",
      "117/117 [==============================] - 0s 786us/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0056 - val_mse: 0.0056\n",
      "Epoch 7/1000\n",
      "117/117 [==============================] - 0s 743us/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 8/1000\n",
      "117/117 [==============================] - 0s 745us/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0051 - val_mse: 0.0051\n",
      "Epoch 9/1000\n",
      "117/117 [==============================] - 0s 753us/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0049 - val_mse: 0.0049\n",
      "Epoch 10/1000\n",
      "117/117 [==============================] - 0s 749us/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0047 - val_mse: 0.0047\n",
      "Epoch 11/1000\n",
      "117/117 [==============================] - 0s 787us/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 12/1000\n",
      "117/117 [==============================] - 0s 727us/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 13/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0043 - val_mse: 0.0043\n",
      "Epoch 14/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0046 - val_mse: 0.0046\n",
      "Epoch 15/1000\n",
      "117/117 [==============================] - 0s 800us/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0041 - val_mse: 0.0041\n",
      "Epoch 16/1000\n",
      "117/117 [==============================] - 0s 799us/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 17/1000\n",
      "117/117 [==============================] - 0s 802us/step - loss: 9.4896e-04 - mse: 9.4896e-04 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 18/1000\n",
      "117/117 [==============================] - 0s 806us/step - loss: 8.8393e-04 - mse: 8.8393e-04 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 19/1000\n",
      "117/117 [==============================] - 0s 760us/step - loss: 9.5987e-04 - mse: 9.5987e-04 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 20/1000\n",
      "117/117 [==============================] - 0s 732us/step - loss: 8.9053e-04 - mse: 8.9053e-04 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 21/1000\n",
      "117/117 [==============================] - ETA: 0s - loss: 8.7647e-04 - mse: 8.7647e-0 - 0s 722us/step - loss: 8.6163e-04 - mse: 8.6163e-04 - val_loss: 0.0044 - val_mse: 0.0044\n",
      "Epoch 22/1000\n",
      "117/117 [==============================] - 0s 785us/step - loss: 8.0838e-04 - mse: 8.0838e-04 - val_loss: 0.0039 - val_mse: 0.0039\n",
      "Epoch 23/1000\n",
      "117/117 [==============================] - 0s 821us/step - loss: 7.6462e-04 - mse: 7.6462e-04 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 24/1000\n",
      "117/117 [==============================] - 0s 779us/step - loss: 8.2612e-04 - mse: 8.2612e-04 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 25/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 7.9198e-04 - mse: 7.9198e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 26/1000\n",
      "117/117 [==============================] - 0s 855us/step - loss: 7.2375e-04 - mse: 7.2375e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 27/1000\n",
      "117/117 [==============================] - 0s 718us/step - loss: 5.5410e-04 - mse: 5.5410e-04 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 28/1000\n",
      "117/117 [==============================] - 0s 755us/step - loss: 5.3137e-04 - mse: 5.3137e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 29/1000\n",
      "117/117 [==============================] - 0s 752us/step - loss: 5.2701e-04 - mse: 5.2701e-04 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 30/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 5.3013e-04 - mse: 5.3013e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 31/1000\n",
      "117/117 [==============================] - 0s 737us/step - loss: 4.9510e-04 - mse: 4.9510e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 32/1000\n",
      "117/117 [==============================] - 0s 764us/step - loss: 4.7340e-04 - mse: 4.7340e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 33/1000\n",
      "117/117 [==============================] - 0s 738us/step - loss: 4.8605e-04 - mse: 4.8605e-04 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 34/1000\n",
      "117/117 [==============================] - 0s 805us/step - loss: 4.5030e-04 - mse: 4.5030e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 35/1000\n",
      "117/117 [==============================] - 0s 777us/step - loss: 4.4092e-04 - mse: 4.4092e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 36/1000\n",
      "117/117 [==============================] - 0s 784us/step - loss: 4.1666e-04 - mse: 4.1666e-04 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 37/1000\n",
      "117/117 [==============================] - 0s 774us/step - loss: 4.8706e-04 - mse: 4.8706e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 38/1000\n",
      "117/117 [==============================] - 0s 726us/step - loss: 3.8022e-04 - mse: 3.8022e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 39/1000\n",
      "117/117 [==============================] - 0s 761us/step - loss: 3.5417e-04 - mse: 3.5417e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 40/1000\n",
      "117/117 [==============================] - 0s 729us/step - loss: 3.6996e-04 - mse: 3.6996e-04 - val_loss: 0.0036 - val_mse: 0.0036\n",
      "Epoch 41/1000\n",
      "117/117 [==============================] - 0s 809us/step - loss: 3.8098e-04 - mse: 3.8098e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 42/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 3.6189e-04 - mse: 3.6189e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 43/1000\n",
      "117/117 [==============================] - 0s 807us/step - loss: 4.4096e-04 - mse: 4.4096e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 44/1000\n",
      "117/117 [==============================] - 0s 727us/step - loss: 2.9718e-04 - mse: 2.9718e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 45/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 3.1502e-04 - mse: 3.1502e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 46/1000\n",
      "117/117 [==============================] - 0s 698us/step - loss: 3.9160e-04 - mse: 3.9160e-04 - val_loss: 0.0031 - val_mse: 0.0031\n",
      "Epoch 47/1000\n",
      "117/117 [==============================] - 0s 770us/step - loss: 3.2168e-04 - mse: 3.2168e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 48/1000\n",
      "117/117 [==============================] - 0s 717us/step - loss: 3.1128e-04 - mse: 3.1128e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 49/1000\n",
      "117/117 [==============================] - 0s 728us/step - loss: 3.0550e-04 - mse: 3.0550e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 50/1000\n",
      "117/117 [==============================] - 0s 718us/step - loss: 5.0380e-04 - mse: 5.0380e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 51/1000\n",
      "117/117 [==============================] - 0s 703us/step - loss: 6.8510e-04 - mse: 6.8510e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 52/1000\n",
      "117/117 [==============================] - 0s 715us/step - loss: 5.2481e-04 - mse: 5.2481e-04 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 53/1000\n",
      "117/117 [==============================] - 0s 735us/step - loss: 4.1496e-04 - mse: 4.1496e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 54/1000\n",
      "117/117 [==============================] - 0s 796us/step - loss: 4.5924e-04 - mse: 4.5924e-04 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 55/1000\n",
      "117/117 [==============================] - 0s 717us/step - loss: 6.0691e-04 - mse: 6.0691e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 56/1000\n",
      "117/117 [==============================] - 0s 752us/step - loss: 4.7281e-04 - mse: 4.7281e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 57/1000\n",
      "117/117 [==============================] - 0s 737us/step - loss: 4.1012e-04 - mse: 4.1012e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 58/1000\n",
      "117/117 [==============================] - 0s 725us/step - loss: 3.4485e-04 - mse: 3.4485e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 59/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 3.7132e-04 - mse: 3.7132e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 60/1000\n",
      "117/117 [==============================] - 0s 686us/step - loss: 2.3708e-04 - mse: 2.3708e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 61/1000\n",
      "117/117 [==============================] - 0s 701us/step - loss: 2.4343e-04 - mse: 2.4343e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 62/1000\n",
      "117/117 [==============================] - 0s 699us/step - loss: 3.2533e-04 - mse: 3.2533e-04 - val_loss: 0.0029 - val_mse: 0.0029\n",
      "Epoch 63/1000\n",
      "117/117 [==============================] - 0s 682us/step - loss: 3.3831e-04 - mse: 3.3831e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 64/1000\n",
      "117/117 [==============================] - 0s 682us/step - loss: 3.1870e-04 - mse: 3.1870e-04 - val_loss: 0.0028 - val_mse: 0.0028\n",
      "Epoch 65/1000\n",
      "117/117 [==============================] - 0s 677us/step - loss: 3.3053e-04 - mse: 3.3053e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 66/1000\n",
      "117/117 [==============================] - 0s 680us/step - loss: 2.4156e-04 - mse: 2.4156e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 67/1000\n",
      "117/117 [==============================] - 0s 701us/step - loss: 2.6188e-04 - mse: 2.6188e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 68/1000\n",
      "117/117 [==============================] - 0s 748us/step - loss: 3.3308e-04 - mse: 3.3308e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 69/1000\n",
      "117/117 [==============================] - 0s 790us/step - loss: 5.3681e-04 - mse: 5.3681e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 70/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 3.7335e-04 - mse: 3.7335e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 71/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 3.3160e-04 - mse: 3.3160e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 72/1000\n",
      "117/117 [==============================] - 0s 786us/step - loss: 3.2183e-04 - mse: 3.2183e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 73/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 2.3349e-04 - mse: 2.3349e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 74/1000\n",
      "117/117 [==============================] - 0s 792us/step - loss: 1.9851e-04 - mse: 1.9851e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 75/1000\n",
      "117/117 [==============================] - 0s 725us/step - loss: 1.7319e-04 - mse: 1.7319e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 76/1000\n",
      "117/117 [==============================] - 0s 839us/step - loss: 1.9138e-04 - mse: 1.9138e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 77/1000\n",
      "117/117 [==============================] - 0s 743us/step - loss: 2.3525e-04 - mse: 2.3525e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 78/1000\n",
      "117/117 [==============================] - 0s 735us/step - loss: 1.9762e-04 - mse: 1.9762e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 79/1000\n",
      "117/117 [==============================] - 0s 806us/step - loss: 2.0473e-04 - mse: 2.0473e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 80/1000\n",
      "117/117 [==============================] - 0s 776us/step - loss: 2.2355e-04 - mse: 2.2355e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 81/1000\n",
      "117/117 [==============================] - 0s 839us/step - loss: 3.0192e-04 - mse: 3.0192e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 82/1000\n",
      "117/117 [==============================] - 0s 838us/step - loss: 2.8130e-04 - mse: 2.8130e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 83/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 3.7435e-04 - mse: 3.7435e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 84/1000\n",
      "117/117 [==============================] - 0s 777us/step - loss: 2.3027e-04 - mse: 2.3027e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 85/1000\n",
      "117/117 [==============================] - 0s 770us/step - loss: 2.2811e-04 - mse: 2.2811e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 86/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 2.4923e-04 - mse: 2.4923e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 87/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 2.3371e-04 - mse: 2.3371e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 88/1000\n",
      "117/117 [==============================] - 0s 733us/step - loss: 1.7890e-04 - mse: 1.7890e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 89/1000\n",
      "117/117 [==============================] - 0s 790us/step - loss: 1.6726e-04 - mse: 1.6726e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 90/1000\n",
      "117/117 [==============================] - 0s 730us/step - loss: 3.1937e-04 - mse: 3.1937e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 91/1000\n",
      "117/117 [==============================] - 0s 824us/step - loss: 3.8723e-04 - mse: 3.8723e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 92/1000\n",
      "117/117 [==============================] - 0s 817us/step - loss: 3.4648e-04 - mse: 3.4648e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 93/1000\n",
      "117/117 [==============================] - 0s 770us/step - loss: 2.7427e-04 - mse: 2.7427e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 94/1000\n",
      "117/117 [==============================] - 0s 766us/step - loss: 1.7611e-04 - mse: 1.7611e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 95/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 1.4116e-04 - mse: 1.4116e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 96/1000\n",
      "117/117 [==============================] - ETA: 0s - loss: 1.1872e-04 - mse: 1.1872e-0 - 0s 725us/step - loss: 1.1758e-04 - mse: 1.1758e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 97/1000\n",
      "117/117 [==============================] - 0s 737us/step - loss: 1.3484e-04 - mse: 1.3484e-04 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 98/1000\n",
      "117/117 [==============================] - 0s 782us/step - loss: 3.8737e-04 - mse: 3.8737e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 99/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 4.5113e-04 - mse: 4.5113e-04 - val_loss: 0.0030 - val_mse: 0.0030\n",
      "Epoch 100/1000\n",
      "117/117 [==============================] - 0s 799us/step - loss: 3.3859e-04 - mse: 3.3859e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 101/1000\n",
      "117/117 [==============================] - 0s 774us/step - loss: 1.6735e-04 - mse: 1.6735e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 102/1000\n",
      "117/117 [==============================] - 0s 764us/step - loss: 1.9416e-04 - mse: 1.9416e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 103/1000\n",
      "117/117 [==============================] - 0s 724us/step - loss: 1.2743e-04 - mse: 1.2743e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 104/1000\n",
      "117/117 [==============================] - 0s 730us/step - loss: 1.2986e-04 - mse: 1.2986e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 105/1000\n",
      "117/117 [==============================] - 0s 763us/step - loss: 1.0936e-04 - mse: 1.0936e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 106/1000\n",
      "117/117 [==============================] - 0s 733us/step - loss: 1.2358e-04 - mse: 1.2358e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 107/1000\n",
      "117/117 [==============================] - 0s 737us/step - loss: 1.3831e-04 - mse: 1.3831e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 108/1000\n",
      "117/117 [==============================] - 0s 758us/step - loss: 1.2654e-04 - mse: 1.2654e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 109/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 1.5074e-04 - mse: 1.5074e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 110/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 2.5502e-04 - mse: 2.5502e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 111/1000\n",
      "117/117 [==============================] - 0s 713us/step - loss: 2.7620e-04 - mse: 2.7620e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 112/1000\n",
      "117/117 [==============================] - 0s 799us/step - loss: 1.9993e-04 - mse: 1.9993e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 113/1000\n",
      "117/117 [==============================] - 0s 847us/step - loss: 2.1185e-04 - mse: 2.1185e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 745us/step - loss: 1.5393e-04 - mse: 1.5393e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 115/1000\n",
      "117/117 [==============================] - 0s 701us/step - loss: 1.9641e-04 - mse: 1.9641e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 116/1000\n",
      "117/117 [==============================] - 0s 699us/step - loss: 1.6470e-04 - mse: 1.6470e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 117/1000\n",
      "117/117 [==============================] - 0s 765us/step - loss: 1.7581e-04 - mse: 1.7581e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 118/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 1.6408e-04 - mse: 1.6408e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 119/1000\n",
      "117/117 [==============================] - 0s 762us/step - loss: 1.2331e-04 - mse: 1.2331e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 120/1000\n",
      "117/117 [==============================] - 0s 791us/step - loss: 1.7069e-04 - mse: 1.7069e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 121/1000\n",
      "117/117 [==============================] - 0s 816us/step - loss: 1.2908e-04 - mse: 1.2908e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 122/1000\n",
      "117/117 [==============================] - 0s 743us/step - loss: 1.1376e-04 - mse: 1.1376e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 123/1000\n",
      "117/117 [==============================] - 0s 928us/step - loss: 1.0009e-04 - mse: 1.0009e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 124/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 1.3031e-04 - mse: 1.3031e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 125/1000\n",
      "117/117 [==============================] - 0s 834us/step - loss: 1.4672e-04 - mse: 1.4672e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 126/1000\n",
      "117/117 [==============================] - 0s 803us/step - loss: 1.1209e-04 - mse: 1.1209e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 127/1000\n",
      "117/117 [==============================] - 0s 750us/step - loss: 2.4153e-04 - mse: 2.4153e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 128/1000\n",
      "117/117 [==============================] - 0s 678us/step - loss: 2.2623e-04 - mse: 2.2623e-04 - val_loss: 0.0025 - val_mse: 0.0025\n",
      "Epoch 129/1000\n",
      "117/117 [==============================] - 0s 713us/step - loss: 1.9660e-04 - mse: 1.9660e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 130/1000\n",
      "117/117 [==============================] - 0s 734us/step - loss: 1.3600e-04 - mse: 1.3600e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 131/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 1.4549e-04 - mse: 1.4549e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 132/1000\n",
      "117/117 [==============================] - 0s 684us/step - loss: 1.0260e-04 - mse: 1.0260e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 133/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 1.3263e-04 - mse: 1.3263e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 134/1000\n",
      "117/117 [==============================] - 0s 757us/step - loss: 2.4534e-04 - mse: 2.4534e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 135/1000\n",
      "117/117 [==============================] - 0s 748us/step - loss: 1.8016e-04 - mse: 1.8016e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 136/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 1.4749e-04 - mse: 1.4749e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 137/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 1.2883e-04 - mse: 1.2883e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 138/1000\n",
      "117/117 [==============================] - 0s 731us/step - loss: 1.4470e-04 - mse: 1.4470e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 139/1000\n",
      "117/117 [==============================] - 0s 759us/step - loss: 2.2291e-04 - mse: 2.2291e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 140/1000\n",
      "117/117 [==============================] - 0s 741us/step - loss: 3.6926e-04 - mse: 3.6926e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 141/1000\n",
      "117/117 [==============================] - 0s 709us/step - loss: 1.5232e-04 - mse: 1.5232e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 142/1000\n",
      "117/117 [==============================] - 0s 745us/step - loss: 9.7865e-05 - mse: 9.7865e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 143/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 8.7374e-05 - mse: 8.7374e-05 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 144/1000\n",
      "117/117 [==============================] - 0s 736us/step - loss: 8.7163e-05 - mse: 8.7163e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 145/1000\n",
      "117/117 [==============================] - 0s 828us/step - loss: 1.2503e-04 - mse: 1.2503e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 146/1000\n",
      "117/117 [==============================] - 0s 795us/step - loss: 1.7030e-04 - mse: 1.7030e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 147/1000\n",
      "117/117 [==============================] - 0s 804us/step - loss: 2.6806e-04 - mse: 2.6806e-04 - val_loss: 0.0024 - val_mse: 0.0024\n",
      "Epoch 148/1000\n",
      "117/117 [==============================] - 0s 741us/step - loss: 1.6323e-04 - mse: 1.6323e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 149/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 9.9306e-05 - mse: 9.9306e-05 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 150/1000\n",
      "117/117 [==============================] - 0s 814us/step - loss: 1.3377e-04 - mse: 1.3377e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 151/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 1.4023e-04 - mse: 1.4023e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 152/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 1.1356e-04 - mse: 1.1356e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 153/1000\n",
      "117/117 [==============================] - 0s 752us/step - loss: 9.0243e-05 - mse: 9.0243e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 154/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 7.9891e-05 - mse: 7.9891e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 155/1000\n",
      "117/117 [==============================] - 0s 824us/step - loss: 7.0564e-05 - mse: 7.0564e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 156/1000\n",
      "117/117 [==============================] - 0s 824us/step - loss: 8.1071e-05 - mse: 8.1071e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 157/1000\n",
      "117/117 [==============================] - 0s 752us/step - loss: 1.0802e-04 - mse: 1.0802e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 158/1000\n",
      "117/117 [==============================] - 0s 715us/step - loss: 9.3252e-05 - mse: 9.3252e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 159/1000\n",
      "117/117 [==============================] - 0s 821us/step - loss: 1.4065e-04 - mse: 1.4065e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 160/1000\n",
      "117/117 [==============================] - 0s 798us/step - loss: 3.4491e-04 - mse: 3.4491e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 161/1000\n",
      "117/117 [==============================] - 0s 797us/step - loss: 2.1937e-04 - mse: 2.1937e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 162/1000\n",
      "117/117 [==============================] - 0s 807us/step - loss: 3.6642e-04 - mse: 3.6642e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 163/1000\n",
      "117/117 [==============================] - 0s 798us/step - loss: 1.6425e-04 - mse: 1.6425e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 164/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 1.6367e-04 - mse: 1.6367e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 165/1000\n",
      "117/117 [==============================] - 0s 727us/step - loss: 1.1492e-04 - mse: 1.1492e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 166/1000\n",
      "117/117 [==============================] - 0s 780us/step - loss: 1.8873e-04 - mse: 1.8873e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 167/1000\n",
      "117/117 [==============================] - 0s 701us/step - loss: 1.1130e-04 - mse: 1.1130e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 168/1000\n",
      "117/117 [==============================] - 0s 795us/step - loss: 1.1622e-04 - mse: 1.1622e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 169/1000\n",
      "117/117 [==============================] - 0s 708us/step - loss: 7.9796e-05 - mse: 7.9796e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 170/1000\n",
      "117/117 [==============================] - 0s 788us/step - loss: 4.9770e-05 - mse: 4.9770e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 171/1000\n",
      "117/117 [==============================] - 0s 709us/step - loss: 4.9108e-05 - mse: 4.9108e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 172/1000\n",
      "117/117 [==============================] - 0s 693us/step - loss: 4.8720e-05 - mse: 4.8720e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 173/1000\n",
      "117/117 [==============================] - 0s 705us/step - loss: 5.6740e-05 - mse: 5.6740e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 174/1000\n",
      "117/117 [==============================] - 0s 747us/step - loss: 6.7162e-05 - mse: 6.7162e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 175/1000\n",
      "117/117 [==============================] - 0s 778us/step - loss: 7.1750e-05 - mse: 7.1750e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 176/1000\n",
      "117/117 [==============================] - 0s 695us/step - loss: 6.1060e-05 - mse: 6.1060e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 177/1000\n",
      "117/117 [==============================] - 0s 707us/step - loss: 7.4654e-05 - mse: 7.4654e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 178/1000\n",
      "117/117 [==============================] - 0s 830us/step - loss: 7.2764e-05 - mse: 7.2764e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 179/1000\n",
      "117/117 [==============================] - 0s 700us/step - loss: 1.3489e-04 - mse: 1.3489e-04 - val_loss: 0.0026 - val_mse: 0.0026\n",
      "Epoch 180/1000\n",
      "117/117 [==============================] - 0s 804us/step - loss: 3.2241e-04 - mse: 3.2241e-04 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 181/1000\n",
      "117/117 [==============================] - 0s 747us/step - loss: 3.6549e-04 - mse: 3.6549e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 182/1000\n",
      "117/117 [==============================] - 0s 836us/step - loss: 2.2344e-04 - mse: 2.2344e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 183/1000\n",
      "117/117 [==============================] - 0s 786us/step - loss: 2.4918e-04 - mse: 2.4918e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 184/1000\n",
      "117/117 [==============================] - 0s 802us/step - loss: 1.9462e-04 - mse: 1.9462e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 185/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 1.1862e-04 - mse: 1.1862e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 186/1000\n",
      "117/117 [==============================] - 0s 771us/step - loss: 9.0287e-05 - mse: 9.0287e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 187/1000\n",
      "117/117 [==============================] - 0s 716us/step - loss: 2.4811e-04 - mse: 2.4811e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 188/1000\n",
      "117/117 [==============================] - 0s 966us/step - loss: 1.2899e-04 - mse: 1.2899e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 189/1000\n",
      "117/117 [==============================] - 0s 828us/step - loss: 7.7083e-05 - mse: 7.7083e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 190/1000\n",
      "117/117 [==============================] - 0s 805us/step - loss: 5.1655e-05 - mse: 5.1655e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 191/1000\n",
      "117/117 [==============================] - 0s 781us/step - loss: 6.0139e-05 - mse: 6.0139e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 192/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 4.6527e-05 - mse: 4.6527e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 193/1000\n",
      "117/117 [==============================] - 0s 785us/step - loss: 3.9557e-05 - mse: 3.9557e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 194/1000\n",
      "117/117 [==============================] - 0s 763us/step - loss: 3.4754e-05 - mse: 3.4754e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 195/1000\n",
      "117/117 [==============================] - 0s 738us/step - loss: 3.6538e-05 - mse: 3.6538e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 196/1000\n",
      "117/117 [==============================] - 0s 782us/step - loss: 1.3111e-04 - mse: 1.3111e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 197/1000\n",
      "117/117 [==============================] - 0s 726us/step - loss: 2.0028e-04 - mse: 2.0028e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 198/1000\n",
      "117/117 [==============================] - 0s 803us/step - loss: 1.3802e-04 - mse: 1.3802e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 199/1000\n",
      "117/117 [==============================] - 0s 787us/step - loss: 1.2758e-04 - mse: 1.2758e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 200/1000\n",
      "117/117 [==============================] - 0s 702us/step - loss: 1.4692e-04 - mse: 1.4692e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 201/1000\n",
      "117/117 [==============================] - 0s 752us/step - loss: 1.6070e-04 - mse: 1.6070e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 202/1000\n",
      "117/117 [==============================] - 0s 727us/step - loss: 1.7623e-04 - mse: 1.7623e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 203/1000\n",
      "117/117 [==============================] - 0s 708us/step - loss: 1.7182e-04 - mse: 1.7182e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 204/1000\n",
      "117/117 [==============================] - 0s 695us/step - loss: 1.4066e-04 - mse: 1.4066e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 205/1000\n",
      "117/117 [==============================] - 0s 760us/step - loss: 8.8485e-05 - mse: 8.8485e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 206/1000\n",
      "117/117 [==============================] - 0s 822us/step - loss: 7.4373e-05 - mse: 7.4373e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 207/1000\n",
      "117/117 [==============================] - 0s 792us/step - loss: 5.1798e-05 - mse: 5.1798e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 208/1000\n",
      "117/117 [==============================] - 0s 741us/step - loss: 4.8617e-05 - mse: 4.8617e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 209/1000\n",
      "117/117 [==============================] - 0s 765us/step - loss: 4.3689e-05 - mse: 4.3689e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 210/1000\n",
      "117/117 [==============================] - 0s 814us/step - loss: 4.7874e-05 - mse: 4.7874e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 211/1000\n",
      "117/117 [==============================] - 0s 727us/step - loss: 5.5421e-05 - mse: 5.5421e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 212/1000\n",
      "117/117 [==============================] - 0s 825us/step - loss: 4.8183e-05 - mse: 4.8183e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 213/1000\n",
      "117/117 [==============================] - 0s 741us/step - loss: 9.0847e-05 - mse: 9.0847e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 214/1000\n",
      "117/117 [==============================] - 0s 813us/step - loss: 1.1487e-04 - mse: 1.1487e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 215/1000\n",
      "117/117 [==============================] - 0s 778us/step - loss: 1.4228e-04 - mse: 1.4228e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 216/1000\n",
      "117/117 [==============================] - 0s 797us/step - loss: 2.1414e-04 - mse: 2.1414e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 217/1000\n",
      "117/117 [==============================] - 0s 792us/step - loss: 1.6232e-04 - mse: 1.6232e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 218/1000\n",
      "117/117 [==============================] - 0s 822us/step - loss: 2.3423e-04 - mse: 2.3423e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 219/1000\n",
      "117/117 [==============================] - 0s 848us/step - loss: 2.1545e-04 - mse: 2.1545e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 220/1000\n",
      "117/117 [==============================] - 0s 802us/step - loss: 1.2871e-04 - mse: 1.2871e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 221/1000\n",
      "117/117 [==============================] - 0s 820us/step - loss: 1.2362e-04 - mse: 1.2362e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 222/1000\n",
      "117/117 [==============================] - 0s 795us/step - loss: 8.2518e-05 - mse: 8.2518e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 223/1000\n",
      "117/117 [==============================] - 0s 718us/step - loss: 5.1933e-05 - mse: 5.1933e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 224/1000\n",
      "117/117 [==============================] - 0s 771us/step - loss: 6.2557e-05 - mse: 6.2557e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 225/1000\n",
      "117/117 [==============================] - 0s 787us/step - loss: 4.5933e-05 - mse: 4.5933e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 226/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 758us/step - loss: 4.3628e-05 - mse: 4.3628e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 227/1000\n",
      "117/117 [==============================] - 0s 759us/step - loss: 3.1759e-05 - mse: 3.1759e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 228/1000\n",
      "117/117 [==============================] - 0s 757us/step - loss: 3.2933e-05 - mse: 3.2933e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 229/1000\n",
      "117/117 [==============================] - 0s 719us/step - loss: 3.2867e-05 - mse: 3.2867e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 230/1000\n",
      "117/117 [==============================] - 0s 744us/step - loss: 4.7117e-05 - mse: 4.7117e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 231/1000\n",
      "117/117 [==============================] - 0s 823us/step - loss: 4.6250e-05 - mse: 4.6250e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 232/1000\n",
      "117/117 [==============================] - 0s 820us/step - loss: 1.2632e-04 - mse: 1.2632e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 233/1000\n",
      "117/117 [==============================] - 0s 803us/step - loss: 1.2801e-04 - mse: 1.2801e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 234/1000\n",
      "117/117 [==============================] - 0s 687us/step - loss: 2.3251e-04 - mse: 2.3251e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 235/1000\n",
      "117/117 [==============================] - 0s 747us/step - loss: 1.2582e-04 - mse: 1.2582e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 236/1000\n",
      "117/117 [==============================] - 0s 768us/step - loss: 1.0024e-04 - mse: 1.0024e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 237/1000\n",
      "117/117 [==============================] - 0s 735us/step - loss: 1.3035e-04 - mse: 1.3035e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 238/1000\n",
      "117/117 [==============================] - 0s 925us/step - loss: 8.4053e-05 - mse: 8.4053e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 239/1000\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 7.0295e-05 - mse: 7.0295e-05 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 240/1000\n",
      "117/117 [==============================] - 0s 809us/step - loss: 9.3941e-05 - mse: 9.3941e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 241/1000\n",
      "117/117 [==============================] - 0s 868us/step - loss: 7.6085e-05 - mse: 7.6085e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 242/1000\n",
      "117/117 [==============================] - 0s 879us/step - loss: 6.6599e-05 - mse: 6.6599e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 243/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 9.5402e-05 - mse: 9.5402e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 244/1000\n",
      "117/117 [==============================] - 0s 728us/step - loss: 1.3857e-04 - mse: 1.3857e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 245/1000\n",
      "117/117 [==============================] - 0s 720us/step - loss: 1.2154e-04 - mse: 1.2154e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 246/1000\n",
      "117/117 [==============================] - 0s 710us/step - loss: 7.8877e-05 - mse: 7.8877e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 247/1000\n",
      "117/117 [==============================] - 0s 804us/step - loss: 1.1331e-04 - mse: 1.1331e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 248/1000\n",
      "117/117 [==============================] - 0s 742us/step - loss: 1.1904e-04 - mse: 1.1904e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 249/1000\n",
      "117/117 [==============================] - 0s 822us/step - loss: 6.2847e-05 - mse: 6.2847e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 250/1000\n",
      "117/117 [==============================] - 0s 805us/step - loss: 6.6976e-05 - mse: 6.6976e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 251/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 6.3932e-05 - mse: 6.3932e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 252/1000\n",
      "117/117 [==============================] - 0s 792us/step - loss: 8.6297e-05 - mse: 8.6297e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 253/1000\n",
      "117/117 [==============================] - 0s 830us/step - loss: 8.0444e-05 - mse: 8.0444e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 254/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 6.2824e-05 - mse: 6.2824e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 255/1000\n",
      "117/117 [==============================] - 0s 849us/step - loss: 1.3298e-04 - mse: 1.3298e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 256/1000\n",
      "117/117 [==============================] - 0s 778us/step - loss: 8.2390e-05 - mse: 8.2390e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 257/1000\n",
      "117/117 [==============================] - 0s 838us/step - loss: 4.7205e-05 - mse: 4.7205e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 258/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 4.8658e-05 - mse: 4.8658e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 259/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 5.2376e-05 - mse: 5.2376e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 260/1000\n",
      "117/117 [==============================] - 0s 754us/step - loss: 5.6485e-05 - mse: 5.6485e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 261/1000\n",
      "117/117 [==============================] - 0s 808us/step - loss: 5.2895e-05 - mse: 5.2895e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 262/1000\n",
      "117/117 [==============================] - 0s 793us/step - loss: 4.9686e-05 - mse: 4.9686e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 263/1000\n",
      "117/117 [==============================] - 0s 765us/step - loss: 5.0225e-05 - mse: 5.0225e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 264/1000\n",
      "117/117 [==============================] - 0s 846us/step - loss: 1.1256e-04 - mse: 1.1256e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 265/1000\n",
      "117/117 [==============================] - 0s 826us/step - loss: 8.2624e-05 - mse: 8.2624e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 266/1000\n",
      "117/117 [==============================] - 0s 766us/step - loss: 7.6959e-05 - mse: 7.6959e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 267/1000\n",
      "117/117 [==============================] - 0s 743us/step - loss: 8.3084e-05 - mse: 8.3084e-05 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 268/1000\n",
      "117/117 [==============================] - 0s 765us/step - loss: 1.4467e-04 - mse: 1.4467e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 269/1000\n",
      "117/117 [==============================] - 0s 776us/step - loss: 1.8685e-04 - mse: 1.8685e-04 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 270/1000\n",
      "117/117 [==============================] - 0s 743us/step - loss: 1.0123e-04 - mse: 1.0123e-04 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 271/1000\n",
      "117/117 [==============================] - 0s 736us/step - loss: 1.1350e-04 - mse: 1.1350e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 272/1000\n",
      "117/117 [==============================] - 0s 747us/step - loss: 8.5293e-05 - mse: 8.5293e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 273/1000\n",
      "117/117 [==============================] - 0s 806us/step - loss: 6.5145e-05 - mse: 6.5145e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 274/1000\n",
      "117/117 [==============================] - 0s 706us/step - loss: 6.0388e-05 - mse: 6.0388e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 275/1000\n",
      "117/117 [==============================] - 0s 806us/step - loss: 6.0911e-05 - mse: 6.0911e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 276/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 6.4788e-05 - mse: 6.4788e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 277/1000\n",
      "117/117 [==============================] - 0s 795us/step - loss: 3.3460e-05 - mse: 3.3460e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 278/1000\n",
      "117/117 [==============================] - 0s 790us/step - loss: 2.8242e-05 - mse: 2.8242e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 279/1000\n",
      "117/117 [==============================] - 0s 743us/step - loss: 4.2276e-05 - mse: 4.2276e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 280/1000\n",
      "117/117 [==============================] - 0s 760us/step - loss: 4.9283e-05 - mse: 4.9283e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 281/1000\n",
      "117/117 [==============================] - 0s 745us/step - loss: 1.4348e-04 - mse: 1.4348e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 282/1000\n",
      "117/117 [==============================] - 0s 822us/step - loss: 1.0859e-04 - mse: 1.0859e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 283/1000\n",
      "117/117 [==============================] - 0s 784us/step - loss: 1.0742e-04 - mse: 1.0742e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 284/1000\n",
      "117/117 [==============================] - 0s 693us/step - loss: 8.2175e-05 - mse: 8.2175e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 285/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 1.0274e-04 - mse: 1.0274e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 286/1000\n",
      "117/117 [==============================] - 0s 802us/step - loss: 5.2748e-05 - mse: 5.2748e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 287/1000\n",
      "117/117 [==============================] - 0s 721us/step - loss: 3.5757e-05 - mse: 3.5757e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 288/1000\n",
      "117/117 [==============================] - 0s 771us/step - loss: 4.1475e-05 - mse: 4.1475e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 289/1000\n",
      "117/117 [==============================] - 0s 774us/step - loss: 4.8607e-05 - mse: 4.8607e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 290/1000\n",
      "117/117 [==============================] - 0s 762us/step - loss: 9.0417e-05 - mse: 9.0417e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 291/1000\n",
      "117/117 [==============================] - 0s 819us/step - loss: 1.0905e-04 - mse: 1.0905e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 292/1000\n",
      "117/117 [==============================] - 0s 821us/step - loss: 7.5496e-05 - mse: 7.5496e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 293/1000\n",
      "117/117 [==============================] - 0s 759us/step - loss: 7.0878e-05 - mse: 7.0878e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 294/1000\n",
      "117/117 [==============================] - 0s 830us/step - loss: 9.7600e-05 - mse: 9.7600e-05 - val_loss: 0.0023 - val_mse: 0.0023\n",
      "Epoch 295/1000\n",
      "117/117 [==============================] - 0s 830us/step - loss: 1.2654e-04 - mse: 1.2654e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 296/1000\n",
      "117/117 [==============================] - 0s 847us/step - loss: 6.5414e-05 - mse: 6.5414e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 297/1000\n",
      "117/117 [==============================] - 0s 755us/step - loss: 9.2048e-05 - mse: 9.2048e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 298/1000\n",
      "117/117 [==============================] - 0s 754us/step - loss: 5.4923e-05 - mse: 5.4923e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 299/1000\n",
      "117/117 [==============================] - 0s 781us/step - loss: 3.7113e-05 - mse: 3.7113e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 300/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 7.1416e-05 - mse: 7.1416e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 301/1000\n",
      "117/117 [==============================] - 0s 820us/step - loss: 5.6030e-05 - mse: 5.6030e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 302/1000\n",
      "117/117 [==============================] - 0s 781us/step - loss: 3.6980e-05 - mse: 3.6980e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 303/1000\n",
      "117/117 [==============================] - 0s 798us/step - loss: 4.2073e-05 - mse: 4.2073e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 304/1000\n",
      "117/117 [==============================] - 0s 815us/step - loss: 7.9505e-05 - mse: 7.9505e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 305/1000\n",
      "117/117 [==============================] - 0s 787us/step - loss: 2.0033e-04 - mse: 2.0033e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 306/1000\n",
      "117/117 [==============================] - 0s 751us/step - loss: 1.3492e-04 - mse: 1.3492e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 307/1000\n",
      "117/117 [==============================] - 0s 798us/step - loss: 8.9313e-05 - mse: 8.9313e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 308/1000\n",
      "117/117 [==============================] - 0s 858us/step - loss: 6.1780e-05 - mse: 6.1780e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 309/1000\n",
      "117/117 [==============================] - 0s 864us/step - loss: 5.3594e-05 - mse: 5.3594e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 310/1000\n",
      "117/117 [==============================] - 0s 828us/step - loss: 7.5390e-05 - mse: 7.5390e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 311/1000\n",
      "117/117 [==============================] - 0s 891us/step - loss: 6.8118e-05 - mse: 6.8118e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 312/1000\n",
      "117/117 [==============================] - 0s 907us/step - loss: 9.0800e-05 - mse: 9.0800e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 313/1000\n",
      "117/117 [==============================] - 0s 785us/step - loss: 5.9831e-05 - mse: 5.9831e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 314/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 6.5714e-05 - mse: 6.5714e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 315/1000\n",
      "117/117 [==============================] - 0s 859us/step - loss: 4.6030e-05 - mse: 4.6030e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 316/1000\n",
      "117/117 [==============================] - 0s 898us/step - loss: 6.2338e-05 - mse: 6.2338e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 317/1000\n",
      "117/117 [==============================] - 0s 775us/step - loss: 5.5734e-05 - mse: 5.5734e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 318/1000\n",
      "117/117 [==============================] - 0s 743us/step - loss: 4.6082e-05 - mse: 4.6082e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 319/1000\n",
      "117/117 [==============================] - 0s 838us/step - loss: 1.6584e-04 - mse: 1.6584e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 320/1000\n",
      "117/117 [==============================] - 0s 739us/step - loss: 7.9679e-05 - mse: 7.9679e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 321/1000\n",
      "117/117 [==============================] - 0s 733us/step - loss: 7.3749e-05 - mse: 7.3749e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 322/1000\n",
      "117/117 [==============================] - 0s 765us/step - loss: 7.2397e-05 - mse: 7.2397e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 323/1000\n",
      "117/117 [==============================] - 0s 726us/step - loss: 4.3412e-05 - mse: 4.3412e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 324/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 2.8443e-05 - mse: 2.8443e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 325/1000\n",
      "117/117 [==============================] - 0s 758us/step - loss: 3.3956e-05 - mse: 3.3956e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 326/1000\n",
      "117/117 [==============================] - 0s 777us/step - loss: 4.2652e-05 - mse: 4.2652e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 327/1000\n",
      "117/117 [==============================] - 0s 822us/step - loss: 4.1452e-05 - mse: 4.1452e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 328/1000\n",
      "117/117 [==============================] - 0s 759us/step - loss: 3.5660e-05 - mse: 3.5660e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 329/1000\n",
      "117/117 [==============================] - 0s 823us/step - loss: 5.8706e-05 - mse: 5.8706e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 330/1000\n",
      "117/117 [==============================] - 0s 908us/step - loss: 9.5281e-05 - mse: 9.5281e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 331/1000\n",
      "117/117 [==============================] - 0s 899us/step - loss: 6.4038e-05 - mse: 6.4038e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 332/1000\n",
      "117/117 [==============================] - 0s 851us/step - loss: 1.0540e-04 - mse: 1.0540e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 333/1000\n",
      "117/117 [==============================] - 0s 873us/step - loss: 5.7825e-05 - mse: 5.7825e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 334/1000\n",
      "117/117 [==============================] - 0s 880us/step - loss: 4.9916e-05 - mse: 4.9916e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 335/1000\n",
      "117/117 [==============================] - 0s 859us/step - loss: 4.1192e-05 - mse: 4.1192e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 336/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 4.7337e-05 - mse: 4.7337e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 337/1000\n",
      "117/117 [==============================] - 0s 752us/step - loss: 5.0186e-05 - mse: 5.0186e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 338/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 741us/step - loss: 1.5901e-04 - mse: 1.5901e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 339/1000\n",
      "117/117 [==============================] - 0s 789us/step - loss: 1.0745e-04 - mse: 1.0745e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 340/1000\n",
      "117/117 [==============================] - 0s 744us/step - loss: 1.5796e-04 - mse: 1.5796e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 341/1000\n",
      "117/117 [==============================] - 0s 742us/step - loss: 4.9807e-05 - mse: 4.9807e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 342/1000\n",
      "117/117 [==============================] - 0s 720us/step - loss: 4.9226e-05 - mse: 4.9226e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 343/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 7.4963e-05 - mse: 7.4963e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 344/1000\n",
      "117/117 [==============================] - 0s 717us/step - loss: 3.7358e-05 - mse: 3.7358e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 345/1000\n",
      "117/117 [==============================] - 0s 728us/step - loss: 2.9789e-05 - mse: 2.9789e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 346/1000\n",
      "117/117 [==============================] - 0s 710us/step - loss: 3.6557e-05 - mse: 3.6557e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 347/1000\n",
      "117/117 [==============================] - 0s 793us/step - loss: 5.1050e-05 - mse: 5.1050e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 348/1000\n",
      "117/117 [==============================] - 0s 796us/step - loss: 4.8594e-05 - mse: 4.8594e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 349/1000\n",
      "117/117 [==============================] - 0s 781us/step - loss: 3.5813e-05 - mse: 3.5813e-05 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 350/1000\n",
      "117/117 [==============================] - 0s 779us/step - loss: 4.0499e-05 - mse: 4.0499e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 351/1000\n",
      "117/117 [==============================] - 0s 776us/step - loss: 5.3232e-05 - mse: 5.3232e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 352/1000\n",
      "117/117 [==============================] - 0s 762us/step - loss: 4.5121e-05 - mse: 4.5121e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 353/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 6.1775e-05 - mse: 6.1775e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 354/1000\n",
      "117/117 [==============================] - 0s 763us/step - loss: 5.8730e-05 - mse: 5.8730e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 355/1000\n",
      "117/117 [==============================] - 0s 818us/step - loss: 1.0371e-04 - mse: 1.0371e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 356/1000\n",
      "117/117 [==============================] - 0s 810us/step - loss: 9.8115e-05 - mse: 9.8115e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 357/1000\n",
      "117/117 [==============================] - 0s 797us/step - loss: 6.5526e-05 - mse: 6.5526e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 358/1000\n",
      "117/117 [==============================] - 0s 728us/step - loss: 5.5561e-05 - mse: 5.5561e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 359/1000\n",
      "117/117 [==============================] - 0s 846us/step - loss: 7.1344e-05 - mse: 7.1344e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 360/1000\n",
      "117/117 [==============================] - 0s 803us/step - loss: 6.0580e-05 - mse: 6.0580e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 361/1000\n",
      "117/117 [==============================] - 0s 744us/step - loss: 6.2182e-05 - mse: 6.2182e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 362/1000\n",
      "117/117 [==============================] - 0s 787us/step - loss: 3.9343e-05 - mse: 3.9343e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 363/1000\n",
      "117/117 [==============================] - 0s 705us/step - loss: 3.2786e-05 - mse: 3.2786e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 364/1000\n",
      "117/117 [==============================] - 0s 788us/step - loss: 5.1267e-05 - mse: 5.1267e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 365/1000\n",
      "117/117 [==============================] - 0s 768us/step - loss: 7.0488e-05 - mse: 7.0488e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 366/1000\n",
      "117/117 [==============================] - 0s 721us/step - loss: 9.0673e-05 - mse: 9.0673e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 367/1000\n",
      "117/117 [==============================] - 0s 709us/step - loss: 5.2472e-05 - mse: 5.2472e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 368/1000\n",
      "117/117 [==============================] - 0s 776us/step - loss: 5.1891e-05 - mse: 5.1891e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 369/1000\n",
      "117/117 [==============================] - 0s 828us/step - loss: 5.1388e-05 - mse: 5.1388e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 370/1000\n",
      "117/117 [==============================] - 0s 738us/step - loss: 5.9656e-05 - mse: 5.9656e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 371/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 6.5634e-05 - mse: 6.5634e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 372/1000\n",
      "117/117 [==============================] - 0s 736us/step - loss: 6.8036e-05 - mse: 6.8036e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 373/1000\n",
      "117/117 [==============================] - 0s 795us/step - loss: 4.1955e-05 - mse: 4.1955e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 374/1000\n",
      "117/117 [==============================] - 0s 773us/step - loss: 2.8621e-05 - mse: 2.8621e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 375/1000\n",
      "117/117 [==============================] - 0s 725us/step - loss: 3.6734e-05 - mse: 3.6734e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 376/1000\n",
      "117/117 [==============================] - 0s 730us/step - loss: 3.9617e-05 - mse: 3.9617e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 377/1000\n",
      "117/117 [==============================] - 0s 753us/step - loss: 5.8814e-05 - mse: 5.8814e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 378/1000\n",
      "117/117 [==============================] - 0s 850us/step - loss: 6.7626e-05 - mse: 6.7626e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 379/1000\n",
      "117/117 [==============================] - 0s 713us/step - loss: 9.4937e-05 - mse: 9.4937e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 380/1000\n",
      "117/117 [==============================] - 0s 815us/step - loss: 5.5329e-05 - mse: 5.5329e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 381/1000\n",
      "117/117 [==============================] - 0s 821us/step - loss: 4.8780e-05 - mse: 4.8780e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 382/1000\n",
      "117/117 [==============================] - 0s 736us/step - loss: 5.2929e-05 - mse: 5.2929e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 383/1000\n",
      "117/117 [==============================] - 0s 735us/step - loss: 3.7533e-05 - mse: 3.7533e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 384/1000\n",
      "117/117 [==============================] - 0s 744us/step - loss: 4.8097e-05 - mse: 4.8097e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 385/1000\n",
      "117/117 [==============================] - 0s 881us/step - loss: 6.3415e-05 - mse: 6.3415e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 386/1000\n",
      "117/117 [==============================] - 0s 860us/step - loss: 5.5259e-05 - mse: 5.5259e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 387/1000\n",
      "117/117 [==============================] - 0s 831us/step - loss: 4.9597e-05 - mse: 4.9597e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 388/1000\n",
      "117/117 [==============================] - 0s 755us/step - loss: 1.0449e-04 - mse: 1.0449e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 389/1000\n",
      "117/117 [==============================] - 0s 749us/step - loss: 9.8620e-05 - mse: 9.8620e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 390/1000\n",
      "117/117 [==============================] - 0s 791us/step - loss: 8.6329e-05 - mse: 8.6329e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 391/1000\n",
      "117/117 [==============================] - 0s 831us/step - loss: 4.6333e-05 - mse: 4.6333e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 392/1000\n",
      "117/117 [==============================] - 0s 804us/step - loss: 4.5555e-05 - mse: 4.5555e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 393/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 3.6218e-05 - mse: 3.6218e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 394/1000\n",
      "117/117 [==============================] - 0s 789us/step - loss: 2.0655e-05 - mse: 2.0655e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 395/1000\n",
      "117/117 [==============================] - 0s 850us/step - loss: 2.6396e-05 - mse: 2.6396e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 396/1000\n",
      "117/117 [==============================] - 0s 764us/step - loss: 2.4662e-05 - mse: 2.4662e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 397/1000\n",
      "117/117 [==============================] - 0s 792us/step - loss: 3.6016e-05 - mse: 3.6016e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 398/1000\n",
      "117/117 [==============================] - 0s 764us/step - loss: 5.2098e-05 - mse: 5.2098e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 399/1000\n",
      "117/117 [==============================] - 0s 776us/step - loss: 4.1628e-05 - mse: 4.1628e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 400/1000\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 9.0338e-05 - mse: 9.0338e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 401/1000\n",
      "117/117 [==============================] - 0s 870us/step - loss: 1.4902e-04 - mse: 1.4902e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 402/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 1.0811e-04 - mse: 1.0811e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 403/1000\n",
      "117/117 [==============================] - 0s 784us/step - loss: 9.6029e-05 - mse: 9.6029e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 404/1000\n",
      "117/117 [==============================] - 0s 799us/step - loss: 5.7785e-05 - mse: 5.7785e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 405/1000\n",
      "117/117 [==============================] - 0s 709us/step - loss: 3.9820e-05 - mse: 3.9820e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 406/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 4.1911e-05 - mse: 4.1911e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 407/1000\n",
      "117/117 [==============================] - 0s 800us/step - loss: 3.6987e-05 - mse: 3.6987e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 408/1000\n",
      "117/117 [==============================] - 0s 778us/step - loss: 4.1324e-05 - mse: 4.1324e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 409/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 3.3393e-05 - mse: 3.3393e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 410/1000\n",
      "117/117 [==============================] - 0s 763us/step - loss: 2.1281e-05 - mse: 2.1281e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 411/1000\n",
      "117/117 [==============================] - 0s 777us/step - loss: 3.3061e-05 - mse: 3.3061e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 412/1000\n",
      "117/117 [==============================] - 0s 968us/step - loss: 4.8844e-05 - mse: 4.8844e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 413/1000\n",
      "117/117 [==============================] - 0s 889us/step - loss: 6.8065e-05 - mse: 6.8065e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 414/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 5.4395e-05 - mse: 5.4395e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 415/1000\n",
      "117/117 [==============================] - 0s 774us/step - loss: 5.7773e-05 - mse: 5.7773e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 416/1000\n",
      "117/117 [==============================] - 0s 773us/step - loss: 4.1079e-05 - mse: 4.1079e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 417/1000\n",
      "117/117 [==============================] - 0s 791us/step - loss: 4.0052e-05 - mse: 4.0052e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 418/1000\n",
      "117/117 [==============================] - 0s 696us/step - loss: 8.3639e-05 - mse: 8.3639e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 419/1000\n",
      "117/117 [==============================] - 0s 762us/step - loss: 6.7989e-05 - mse: 6.7989e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 420/1000\n",
      "117/117 [==============================] - 0s 763us/step - loss: 5.0826e-05 - mse: 5.0826e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 421/1000\n",
      "117/117 [==============================] - 0s 842us/step - loss: 5.9935e-05 - mse: 5.9935e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 422/1000\n",
      "117/117 [==============================] - 0s 727us/step - loss: 4.2129e-05 - mse: 4.2129e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 423/1000\n",
      "117/117 [==============================] - 0s 797us/step - loss: 2.9044e-05 - mse: 2.9044e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 424/1000\n",
      "117/117 [==============================] - ETA: 0s - loss: 2.4160e-05 - mse: 2.4160e-0 - 0s 701us/step - loss: 2.4043e-05 - mse: 2.4043e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 425/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 2.1035e-05 - mse: 2.1035e-05 - val_loss: 0.0017 - val_mse: 0.0017\n",
      "Epoch 426/1000\n",
      "117/117 [==============================] - 0s 843us/step - loss: 3.6334e-05 - mse: 3.6334e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 427/1000\n",
      "117/117 [==============================] - 0s 760us/step - loss: 5.6209e-05 - mse: 5.6209e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 428/1000\n",
      "117/117 [==============================] - 0s 730us/step - loss: 5.0348e-05 - mse: 5.0348e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 429/1000\n",
      "117/117 [==============================] - 0s 729us/step - loss: 8.3463e-05 - mse: 8.3463e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 430/1000\n",
      "117/117 [==============================] - 0s 749us/step - loss: 9.0216e-05 - mse: 9.0216e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 431/1000\n",
      "117/117 [==============================] - 0s 767us/step - loss: 1.3634e-04 - mse: 1.3634e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 432/1000\n",
      "117/117 [==============================] - 0s 826us/step - loss: 6.3561e-05 - mse: 6.3561e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 433/1000\n",
      "117/117 [==============================] - 0s 720us/step - loss: 5.9169e-05 - mse: 5.9169e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 434/1000\n",
      "117/117 [==============================] - 0s 833us/step - loss: 2.9697e-05 - mse: 2.9697e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 435/1000\n",
      "117/117 [==============================] - 0s 714us/step - loss: 2.3001e-05 - mse: 2.3001e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 436/1000\n",
      "117/117 [==============================] - 0s 813us/step - loss: 2.2889e-05 - mse: 2.2889e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 437/1000\n",
      "117/117 [==============================] - 0s 735us/step - loss: 1.6320e-05 - mse: 1.6320e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 438/1000\n",
      "117/117 [==============================] - 0s 738us/step - loss: 1.6675e-05 - mse: 1.6675e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 439/1000\n",
      "117/117 [==============================] - 0s 723us/step - loss: 2.6479e-05 - mse: 2.6479e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 440/1000\n",
      "117/117 [==============================] - 0s 755us/step - loss: 4.2037e-05 - mse: 4.2037e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 441/1000\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 4.3837e-05 - mse: 4.3837e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 442/1000\n",
      "117/117 [==============================] - 0s 949us/step - loss: 3.2718e-05 - mse: 3.2718e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 443/1000\n",
      "117/117 [==============================] - 0s 754us/step - loss: 8.0600e-05 - mse: 8.0600e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 444/1000\n",
      "117/117 [==============================] - 0s 823us/step - loss: 7.3238e-05 - mse: 7.3238e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 445/1000\n",
      "117/117 [==============================] - 0s 851us/step - loss: 4.2786e-05 - mse: 4.2786e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 446/1000\n",
      "117/117 [==============================] - 0s 827us/step - loss: 5.3379e-05 - mse: 5.3379e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 447/1000\n",
      "117/117 [==============================] - 0s 784us/step - loss: 7.2783e-05 - mse: 7.2783e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 448/1000\n",
      "117/117 [==============================] - 0s 795us/step - loss: 6.0479e-05 - mse: 6.0479e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 449/1000\n",
      "117/117 [==============================] - 0s 749us/step - loss: 4.5504e-05 - mse: 4.5504e-05 - val_loss: 0.0019 - val_mse: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450/1000\n",
      "117/117 [==============================] - 0s 795us/step - loss: 4.1642e-05 - mse: 4.1642e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 451/1000\n",
      "117/117 [==============================] - 0s 801us/step - loss: 3.6930e-05 - mse: 3.6930e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 452/1000\n",
      "117/117 [==============================] - 0s 745us/step - loss: 2.5360e-05 - mse: 2.5360e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 453/1000\n",
      "117/117 [==============================] - 0s 789us/step - loss: 3.8849e-05 - mse: 3.8849e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 454/1000\n",
      "117/117 [==============================] - 0s 753us/step - loss: 6.4034e-05 - mse: 6.4034e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 455/1000\n",
      "117/117 [==============================] - 0s 708us/step - loss: 5.5088e-05 - mse: 5.5088e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 456/1000\n",
      "117/117 [==============================] - 0s 707us/step - loss: 5.3125e-05 - mse: 5.3125e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 457/1000\n",
      "117/117 [==============================] - 0s 741us/step - loss: 4.8462e-05 - mse: 4.8462e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 458/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 3.6580e-05 - mse: 3.6580e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 459/1000\n",
      "117/117 [==============================] - 0s 751us/step - loss: 3.8111e-05 - mse: 3.8111e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 460/1000\n",
      "117/117 [==============================] - 0s 788us/step - loss: 3.6161e-05 - mse: 3.6161e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 461/1000\n",
      "117/117 [==============================] - 0s 826us/step - loss: 3.5645e-05 - mse: 3.5645e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 462/1000\n",
      "117/117 [==============================] - 0s 789us/step - loss: 2.6025e-05 - mse: 2.6025e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 463/1000\n",
      "117/117 [==============================] - 0s 778us/step - loss: 3.4142e-05 - mse: 3.4142e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 464/1000\n",
      "117/117 [==============================] - 0s 821us/step - loss: 6.5328e-05 - mse: 6.5328e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 465/1000\n",
      "117/117 [==============================] - 0s 759us/step - loss: 4.2138e-05 - mse: 4.2138e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 466/1000\n",
      "117/117 [==============================] - 0s 781us/step - loss: 4.2755e-05 - mse: 4.2755e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 467/1000\n",
      "117/117 [==============================] - 0s 815us/step - loss: 2.2869e-05 - mse: 2.2869e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 468/1000\n",
      "117/117 [==============================] - 0s 761us/step - loss: 2.8834e-05 - mse: 2.8834e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 469/1000\n",
      "117/117 [==============================] - 0s 813us/step - loss: 3.1281e-05 - mse: 3.1281e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 470/1000\n",
      "117/117 [==============================] - 0s 846us/step - loss: 7.2059e-05 - mse: 7.2059e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 471/1000\n",
      "117/117 [==============================] - 0s 840us/step - loss: 7.4018e-05 - mse: 7.4018e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 472/1000\n",
      "117/117 [==============================] - 0s 805us/step - loss: 5.1684e-05 - mse: 5.1684e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 473/1000\n",
      "117/117 [==============================] - 0s 803us/step - loss: 3.7670e-05 - mse: 3.7670e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 474/1000\n",
      "117/117 [==============================] - 0s 783us/step - loss: 6.1098e-05 - mse: 6.1098e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 475/1000\n",
      "117/117 [==============================] - 0s 804us/step - loss: 6.2064e-05 - mse: 6.2064e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 476/1000\n",
      "117/117 [==============================] - 0s 852us/step - loss: 3.7497e-05 - mse: 3.7497e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 477/1000\n",
      "117/117 [==============================] - 0s 831us/step - loss: 1.3449e-04 - mse: 1.3449e-04 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 478/1000\n",
      "117/117 [==============================] - 0s 750us/step - loss: 6.0771e-05 - mse: 6.0771e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 479/1000\n",
      "117/117 [==============================] - 0s 842us/step - loss: 4.7713e-05 - mse: 4.7713e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 480/1000\n",
      "117/117 [==============================] - 0s 847us/step - loss: 2.8454e-05 - mse: 2.8454e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 481/1000\n",
      "117/117 [==============================] - 0s 765us/step - loss: 1.7725e-05 - mse: 1.7725e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 482/1000\n",
      "117/117 [==============================] - 0s 759us/step - loss: 1.5398e-05 - mse: 1.5398e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 483/1000\n",
      "117/117 [==============================] - 0s 815us/step - loss: 1.4380e-05 - mse: 1.4380e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 484/1000\n",
      "117/117 [==============================] - 0s 773us/step - loss: 2.4987e-05 - mse: 2.4987e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 485/1000\n",
      "117/117 [==============================] - 0s 805us/step - loss: 3.0624e-05 - mse: 3.0624e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 486/1000\n",
      "117/117 [==============================] - 0s 810us/step - loss: 7.6554e-05 - mse: 7.6554e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 487/1000\n",
      "117/117 [==============================] - 0s 848us/step - loss: 1.1440e-04 - mse: 1.1440e-04 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 488/1000\n",
      "117/117 [==============================] - 0s 814us/step - loss: 7.9296e-05 - mse: 7.9296e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 489/1000\n",
      "117/117 [==============================] - 0s 804us/step - loss: 4.5888e-05 - mse: 4.5888e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 490/1000\n",
      "117/117 [==============================] - 0s 831us/step - loss: 3.5943e-05 - mse: 3.5943e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 491/1000\n",
      "117/117 [==============================] - 0s 766us/step - loss: 2.5431e-05 - mse: 2.5431e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 492/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 6.5442e-05 - mse: 6.5442e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 493/1000\n",
      "117/117 [==============================] - 0s 810us/step - loss: 3.2698e-05 - mse: 3.2698e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 494/1000\n",
      "117/117 [==============================] - 0s 805us/step - loss: 2.0837e-05 - mse: 2.0837e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 495/1000\n",
      "117/117 [==============================] - 0s 751us/step - loss: 1.5994e-05 - mse: 1.5994e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 496/1000\n",
      "117/117 [==============================] - 0s 806us/step - loss: 2.4193e-05 - mse: 2.4193e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 497/1000\n",
      "117/117 [==============================] - 0s 742us/step - loss: 2.8687e-05 - mse: 2.8687e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 498/1000\n",
      "117/117 [==============================] - 0s 837us/step - loss: 3.7624e-05 - mse: 3.7624e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 499/1000\n",
      "117/117 [==============================] - 0s 793us/step - loss: 3.0123e-05 - mse: 3.0123e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 500/1000\n",
      "117/117 [==============================] - 0s 761us/step - loss: 3.5496e-05 - mse: 3.5496e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 501/1000\n",
      "117/117 [==============================] - 0s 777us/step - loss: 6.7155e-05 - mse: 6.7155e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 502/1000\n",
      "117/117 [==============================] - 0s 784us/step - loss: 8.3390e-05 - mse: 8.3390e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 503/1000\n",
      "117/117 [==============================] - 0s 813us/step - loss: 5.6775e-05 - mse: 5.6775e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 504/1000\n",
      "117/117 [==============================] - 0s 764us/step - loss: 6.9843e-05 - mse: 6.9843e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 505/1000\n",
      "117/117 [==============================] - 0s 777us/step - loss: 3.3461e-05 - mse: 3.3461e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 506/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 3.6099e-05 - mse: 3.6099e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 507/1000\n",
      "117/117 [==============================] - 0s 738us/step - loss: 4.7924e-05 - mse: 4.7924e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 508/1000\n",
      "117/117 [==============================] - 0s 733us/step - loss: 2.6547e-05 - mse: 2.6547e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 509/1000\n",
      "117/117 [==============================] - 0s 784us/step - loss: 1.8597e-05 - mse: 1.8597e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 510/1000\n",
      "117/117 [==============================] - 0s 736us/step - loss: 1.7763e-05 - mse: 1.7763e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 511/1000\n",
      "117/117 [==============================] - 0s 785us/step - loss: 1.5399e-05 - mse: 1.5399e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 512/1000\n",
      "117/117 [==============================] - 0s 788us/step - loss: 2.9575e-05 - mse: 2.9575e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 513/1000\n",
      "117/117 [==============================] - 0s 810us/step - loss: 4.3443e-05 - mse: 4.3443e-05 - val_loss: 0.0021 - val_mse: 0.0021\n",
      "Epoch 514/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 1.1938e-04 - mse: 1.1938e-04 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 515/1000\n",
      "117/117 [==============================] - 0s 770us/step - loss: 7.1120e-05 - mse: 7.1120e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 516/1000\n",
      "117/117 [==============================] - 0s 786us/step - loss: 3.6424e-05 - mse: 3.6424e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 517/1000\n",
      "117/117 [==============================] - 0s 815us/step - loss: 2.4916e-05 - mse: 2.4916e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 518/1000\n",
      "117/117 [==============================] - 0s 838us/step - loss: 2.9505e-05 - mse: 2.9505e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 519/1000\n",
      "117/117 [==============================] - 0s 769us/step - loss: 2.4188e-05 - mse: 2.4188e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 520/1000\n",
      "117/117 [==============================] - 0s 806us/step - loss: 2.1401e-05 - mse: 2.1401e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 521/1000\n",
      "117/117 [==============================] - 0s 784us/step - loss: 3.0176e-05 - mse: 3.0176e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 522/1000\n",
      "117/117 [==============================] - 0s 768us/step - loss: 5.9317e-05 - mse: 5.9317e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 523/1000\n",
      "117/117 [==============================] - 0s 829us/step - loss: 3.5525e-05 - mse: 3.5525e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 524/1000\n",
      "117/117 [==============================] - 0s 845us/step - loss: 2.8615e-05 - mse: 2.8615e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 525/1000\n",
      "117/117 [==============================] - 0s 796us/step - loss: 3.2158e-05 - mse: 3.2158e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 526/1000\n",
      "117/117 [==============================] - 0s 789us/step - loss: 2.5087e-05 - mse: 2.5087e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 527/1000\n",
      "117/117 [==============================] - 0s 812us/step - loss: 3.5235e-05 - mse: 3.5235e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 528/1000\n",
      "117/117 [==============================] - 0s 828us/step - loss: 3.6057e-05 - mse: 3.6057e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 529/1000\n",
      "117/117 [==============================] - 0s 751us/step - loss: 6.4509e-05 - mse: 6.4509e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 530/1000\n",
      "117/117 [==============================] - 0s 840us/step - loss: 6.7779e-05 - mse: 6.7779e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 531/1000\n",
      "117/117 [==============================] - 0s 812us/step - loss: 5.2333e-05 - mse: 5.2333e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 532/1000\n",
      "117/117 [==============================] - 0s 761us/step - loss: 4.1114e-05 - mse: 4.1114e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 533/1000\n",
      "117/117 [==============================] - 0s 820us/step - loss: 3.2150e-05 - mse: 3.2150e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 534/1000\n",
      "117/117 [==============================] - 0s 787us/step - loss: 2.5274e-05 - mse: 2.5274e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 535/1000\n",
      "117/117 [==============================] - 0s 772us/step - loss: 2.0872e-05 - mse: 2.0872e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 536/1000\n",
      "117/117 [==============================] - 0s 750us/step - loss: 2.0177e-05 - mse: 2.0177e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 537/1000\n",
      "117/117 [==============================] - 0s 849us/step - loss: 2.2738e-05 - mse: 2.2738e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 538/1000\n",
      "117/117 [==============================] - 0s 786us/step - loss: 4.5429e-05 - mse: 4.5429e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 539/1000\n",
      "117/117 [==============================] - 0s 772us/step - loss: 5.8955e-05 - mse: 5.8955e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 540/1000\n",
      "117/117 [==============================] - 0s 836us/step - loss: 7.3366e-05 - mse: 7.3366e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 541/1000\n",
      "117/117 [==============================] - 0s 812us/step - loss: 7.5630e-05 - mse: 7.5630e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 542/1000\n",
      "117/117 [==============================] - 0s 803us/step - loss: 5.7990e-05 - mse: 5.7990e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 543/1000\n",
      "117/117 [==============================] - 0s 879us/step - loss: 3.1643e-05 - mse: 3.1643e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 544/1000\n",
      "117/117 [==============================] - 0s 838us/step - loss: 2.7742e-05 - mse: 2.7742e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 545/1000\n",
      "117/117 [==============================] - 0s 831us/step - loss: 2.0440e-05 - mse: 2.0440e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 546/1000\n",
      "117/117 [==============================] - 0s 793us/step - loss: 2.5840e-05 - mse: 2.5840e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 547/1000\n",
      "117/117 [==============================] - 0s 794us/step - loss: 3.5666e-05 - mse: 3.5666e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 548/1000\n",
      "117/117 [==============================] - 0s 740us/step - loss: 2.2829e-05 - mse: 2.2829e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 549/1000\n",
      "117/117 [==============================] - 0s 821us/step - loss: 2.6339e-05 - mse: 2.6339e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 550/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 4.1969e-05 - mse: 4.1969e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 551/1000\n",
      "117/117 [==============================] - 0s 828us/step - loss: 5.1108e-05 - mse: 5.1108e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 552/1000\n",
      "117/117 [==============================] - 0s 720us/step - loss: 2.4162e-05 - mse: 2.4162e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 553/1000\n",
      "117/117 [==============================] - 0s 826us/step - loss: 2.9762e-05 - mse: 2.9762e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 554/1000\n",
      "117/117 [==============================] - 0s 868us/step - loss: 4.4851e-05 - mse: 4.4851e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 555/1000\n",
      "117/117 [==============================] - 0s 732us/step - loss: 3.7520e-05 - mse: 3.7520e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 556/1000\n",
      "117/117 [==============================] - 0s 766us/step - loss: 4.7730e-05 - mse: 4.7730e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 557/1000\n",
      "117/117 [==============================] - 0s 748us/step - loss: 4.7329e-05 - mse: 4.7329e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 558/1000\n",
      "117/117 [==============================] - 0s 823us/step - loss: 4.3169e-05 - mse: 4.3169e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 559/1000\n",
      "117/117 [==============================] - 0s 811us/step - loss: 4.1970e-05 - mse: 4.1970e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 560/1000\n",
      "117/117 [==============================] - 0s 750us/step - loss: 3.4762e-05 - mse: 3.4762e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 561/1000\n",
      "117/117 [==============================] - 0s 819us/step - loss: 1.8082e-05 - mse: 1.8082e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 562/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 0s 746us/step - loss: 1.4349e-05 - mse: 1.4349e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 563/1000\n",
      "117/117 [==============================] - 0s 788us/step - loss: 1.7271e-05 - mse: 1.7271e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 564/1000\n",
      "117/117 [==============================] - 0s 775us/step - loss: 2.2834e-05 - mse: 2.2834e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 565/1000\n",
      "117/117 [==============================] - 0s 787us/step - loss: 3.3459e-05 - mse: 3.3459e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 566/1000\n",
      "117/117 [==============================] - 0s 751us/step - loss: 2.9787e-05 - mse: 2.9787e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 567/1000\n",
      "117/117 [==============================] - 0s 786us/step - loss: 2.1629e-05 - mse: 2.1629e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 568/1000\n",
      "117/117 [==============================] - 0s 805us/step - loss: 2.7145e-05 - mse: 2.7145e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 569/1000\n",
      "117/117 [==============================] - 0s 821us/step - loss: 3.2080e-05 - mse: 3.2080e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 570/1000\n",
      "117/117 [==============================] - 0s 806us/step - loss: 3.5281e-05 - mse: 3.5281e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 571/1000\n",
      "117/117 [==============================] - 0s 745us/step - loss: 4.0646e-05 - mse: 4.0646e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 572/1000\n",
      "117/117 [==============================] - 0s 777us/step - loss: 4.7191e-05 - mse: 4.7191e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 573/1000\n",
      "117/117 [==============================] - 0s 826us/step - loss: 3.7112e-05 - mse: 3.7112e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 574/1000\n",
      "117/117 [==============================] - 0s 785us/step - loss: 3.3878e-05 - mse: 3.3878e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 575/1000\n",
      "117/117 [==============================] - 0s 813us/step - loss: 2.2329e-05 - mse: 2.2329e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 576/1000\n",
      "117/117 [==============================] - 0s 840us/step - loss: 5.3819e-05 - mse: 5.3819e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 577/1000\n",
      "117/117 [==============================] - 0s 763us/step - loss: 6.4656e-05 - mse: 6.4656e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 578/1000\n",
      "117/117 [==============================] - 0s 780us/step - loss: 3.7499e-05 - mse: 3.7499e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 579/1000\n",
      "117/117 [==============================] - 0s 835us/step - loss: 3.6902e-05 - mse: 3.6902e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 580/1000\n",
      "117/117 [==============================] - 0s 746us/step - loss: 2.1733e-05 - mse: 2.1733e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 581/1000\n",
      "117/117 [==============================] - 0s 833us/step - loss: 1.5326e-05 - mse: 1.5326e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 582/1000\n",
      "117/117 [==============================] - 0s 844us/step - loss: 1.3666e-05 - mse: 1.3666e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 583/1000\n",
      "117/117 [==============================] - 0s 785us/step - loss: 1.7892e-05 - mse: 1.7892e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 584/1000\n",
      "117/117 [==============================] - 0s 789us/step - loss: 1.9741e-05 - mse: 1.9741e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 585/1000\n",
      "117/117 [==============================] - 0s 830us/step - loss: 1.8652e-05 - mse: 1.8652e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 586/1000\n",
      "117/117 [==============================] - 0s 843us/step - loss: 3.3887e-05 - mse: 3.3887e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 587/1000\n",
      "117/117 [==============================] - 0s 839us/step - loss: 6.3547e-05 - mse: 6.3547e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 588/1000\n",
      "117/117 [==============================] - 0s 842us/step - loss: 5.6643e-05 - mse: 5.6643e-05 - val_loss: 0.0020 - val_mse: 0.0020\n",
      "Epoch 589/1000\n",
      "117/117 [==============================] - 0s 740us/step - loss: 4.3506e-05 - mse: 4.3506e-05 - val_loss: 0.0019 - val_mse: 0.0019\n",
      "Epoch 590/1000\n",
      "117/117 [==============================] - 0s 846us/step - loss: 3.6846e-05 - mse: 3.6846e-05 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 00590: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1596453bb88>"
      ]
     },
     "execution_count": 1058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(units=50, kernel_initializer='he_uniform', activation='relu', input_dim = 111))\n",
    "\n",
    "ann.add(Dense(units=25,kernel_initializer = 'he_uniform', activation='relu'))\n",
    "\n",
    "ann.add(Dense(units=50,kernel_initializer = 'he_uniform', activation='relu'))\n",
    "\n",
    "ann.add(Dense(units=1,kernel_initializer = 'he_uniform'))\n",
    "\n",
    "ann.compile(loss='mean_squared_error', optimizer= 'adam', metrics=['mse'])\n",
    "\n",
    "\n",
    "\n",
    "ann.fit(X_train,y_train, validation_split= 0.2,epochs=1000, batch_size=10,  callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15964742a88>"
      ]
     },
     "execution_count": 1059,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(ann.history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807201754902942"
      ]
     },
     "execution_count": 1061,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = ann.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = sc_y.inverse_transform(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[115315.76],\n",
       "       [196202.98],\n",
       "       [179873.95],\n",
       "       ...,\n",
       "       [159960.39],\n",
       "       [108075.37],\n",
       "       [214863.56]], dtype=float32)"
      ]
     },
     "execution_count": 1064,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('sample_submission_init.csv')\n",
    "df_sample['SalePrice_pred'] = pd.Series(df_pred.ravel())\n",
    "df_sample=df_sample[['Id','SalePrice_pred']].rename(columns = {'SalePrice_pred':'SalePrice'})\n",
    "df_sample.to_csv('sample_submission.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score is 0.17243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on ANN model ( deep learning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer):\n",
    "    ann =  Sequential()\n",
    "    ann.add(Dense(50, input_dim = 111,activation='relu'))\n",
    "    ann.add(Dense(25, activation='relu'))\n",
    "    ann.add(Dense(1))\n",
    "    ann.compile(loss= 'mean_squared_error', optimizer= optimizer, metrics=['mse'])\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasRegressor(build_fn=create_model(optimizer), verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-da622e05693a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mparams_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'get_params'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0mappend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0mappend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m# If is its own copy, don't memoize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__setstate__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__reduce_ex__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mreductor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m                         \u001b[0mrv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreductor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                         \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__reduce__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "batch_size = [ 20 ]\n",
    "epochs = [1000]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "# learning_rate = [0.001, 0.01, 0.1]\n",
    "momentum = [ 0.2, 0.6, 0.9]\n",
    "\n",
    "\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs,optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': [20],\n",
       " 'epochs': [1000],\n",
       " 'optimizer': ['SGD',\n",
       "  'RMSprop',\n",
       "  'Adagrad',\n",
       "  'Adadelta',\n",
       "  'Adam',\n",
       "  'Adamax',\n",
       "  'Nadam']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.002062 using {'batch_size': 20, 'epochs': 500}\n",
      "-0.002668 (0.000360) with: {'batch_size': 10, 'epochs': 100}\n",
      "-0.002111 (0.000030) with: {'batch_size': 10, 'epochs': 500}\n",
      "-0.002179 (0.000228) with: {'batch_size': 10, 'epochs': 1000}\n",
      "-0.003137 (0.000194) with: {'batch_size': 20, 'epochs': 100}\n",
      "-0.002062 (0.000063) with: {'batch_size': 20, 'epochs': 500}\n",
      "-0.002265 (0.000215) with: {'batch_size': 20, 'epochs': 1000}\n",
      "-0.003916 (0.000465) with: {'batch_size': 40, 'epochs': 100}\n",
      "-0.002638 (0.000179) with: {'batch_size': 40, 'epochs': 500}\n",
      "-0.002573 (0.000334) with: {'batch_size': 40, 'epochs': 1000}\n",
      "-0.003910 (0.000288) with: {'batch_size': 60, 'epochs': 100}\n",
      "-0.003093 (0.000066) with: {'batch_size': 60, 'epochs': 500}\n",
      "-0.002867 (0.000276) with: {'batch_size': 60, 'epochs': 1000}\n",
      "-0.003533 (0.000696) with: {'batch_size': 80, 'epochs': 100}\n",
      "-0.004095 (0.000185) with: {'batch_size': 80, 'epochs': 500}\n",
      "-0.002966 (0.000292) with: {'batch_size': 80, 'epochs': 1000}\n",
      "-0.004533 (0.000179) with: {'batch_size': 100, 'epochs': 100}\n",
      "-0.004966 (0.000519) with: {'batch_size': 100, 'epochs': 500}\n",
      "-0.003806 (0.000326) with: {'batch_size': 100, 'epochs': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Nadam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0069 - mse: 0.0110 - val_loss: 0.0022 - val_mse: 0.0035\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.0021 - mse: 0.0038 - val_loss: 0.0012 - val_mse: 0.0020\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0016 - mse: 0.0029 - val_loss: 0.0023 - val_mse: 0.0037\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0015 - mse: 0.0026 - val_loss: 0.0010 - val_mse: 0.0017\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0013 - mse: 0.0023 - val_loss: 0.0010 - val_mse: 0.0017\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0013 - mse: 0.0023 - val_loss: 0.0013 - val_mse: 0.0022\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0011 - mse: 0.0020 - val_loss: 9.7278e-04 - val_mse: 0.0016\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 9.9931e-04 - mse: 0.0018 - val_loss: 0.0010 - val_mse: 0.0016\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 9.2717e-04 - mse: 0.0016 - val_loss: 0.0010 - val_mse: 0.0016\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 9.4015e-04 - mse: 0.0017 - val_loss: 9.2088e-04 - val_mse: 0.0015\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 7.9515e-04 - mse: 0.0014 - val_loss: 0.0010 - val_mse: 0.0016\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 8.0770e-04 - mse: 0.0014 - val_loss: 9.9612e-04 - val_mse: 0.0016\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 6.1555e-04 - mse: 0.0011 - val_loss: 0.0010 - val_mse: 0.0016\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 6.4797e-04 - mse: 0.0011 - val_loss: 9.0417e-04 - val_mse: 0.0015\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 5.7262e-04 - mse: 9.6603e-04 - val_loss: 0.0011 - val_mse: 0.0017\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 5.5401e-04 - mse: 9.3374e-04 - val_loss: 9.4375e-04 - val_mse: 0.0016\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 5.3937e-04 - mse: 9.4800e-04 - val_loss: 9.7827e-04 - val_mse: 0.0016\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.8629e-04 - mse: 8.0533e-04 - val_loss: 8.5139e-04 - val_mse: 0.0014\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.2296e-04 - mse: 7.2011e-04 - val_loss: 0.0013 - val_mse: 0.0021\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.7704e-04 - mse: 7.8954e-04 - val_loss: 0.0010 - val_mse: 0.0018\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.3485e-04 - mse: 7.2530e-04 - val_loss: 0.0012 - val_mse: 0.0021\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.1989e-04 - mse: 7.1576e-04 - val_loss: 9.7899e-04 - val_mse: 0.0016\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.9351e-04 - mse: 8.3753e-04 - val_loss: 9.4514e-04 - val_mse: 0.0016\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 4.0622e-04 - mse: 6.9142e-04 - val_loss: 0.0010 - val_mse: 0.0018\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.3234e-04 - mse: 7.4350e-04 - val_loss: 0.0011 - val_mse: 0.0018\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.2831e-04 - mse: 5.2631e-04 - val_loss: 9.9989e-04 - val_mse: 0.0017\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 4.1597e-04 - mse: 7.3347e-04 - val_loss: 9.2728e-04 - val_mse: 0.0016\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.8007e-04 - mse: 6.2380e-04 - val_loss: 9.8551e-04 - val_mse: 0.0016\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.0706e-04 - mse: 5.2446e-04 - val_loss: 0.0011 - val_mse: 0.0018\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.3749e-04 - mse: 5.8703e-04 - val_loss: 9.8760e-04 - val_mse: 0.0016\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.2749e-04 - mse: 5.6025e-04 - val_loss: 0.0010 - val_mse: 0.0017\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.2300e-04 - mse: 5.2924e-04 - val_loss: 9.7741e-04 - val_mse: 0.0016\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 3.1039e-04 - mse: 5.0203e-04 - val_loss: 0.0012 - val_mse: 0.0020\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.6304e-04 - mse: 6.4472e-04 - val_loss: 9.4841e-04 - val_mse: 0.0016\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 3.3615e-04 - mse: 5.6339e-04 - val_loss: 0.0010 - val_mse: 0.0017\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 2.7328e-04 - mse: 4.5508e-04 - val_loss: 9.7142e-04 - val_mse: 0.0016\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 2.4618e-04 - mse: 3.9817e-04 - val_loss: 0.0011 - val_mse: 0.0019\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 5ms/step - loss: 2.4668e-04 - mse: 4.0470e-04 - val_loss: 0.0012 - val_mse: 0.0021\n",
      "Epoch 00038: early stopping\n",
      "Best Accuracy for -0.0014779039192944765 using {'batch_size': 20, 'drop_out': 0.1, 'epochs': 50, 'init_mode': 'normal', 'loss': 'mean_squared_logarithmic_error', 'optimizer': 'Nadam'}\n",
      " mean=-0.001478, std=0.0001658 using {'batch_size': 20, 'drop_out': 0.1, 'epochs': 50, 'init_mode': 'normal', 'loss': 'mean_squared_logarithmic_error', 'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "\n",
    "\n",
    "early_stop = EarlyStopping( monitor='val_loss',verbose=1, patience=20)\n",
    "\n",
    "def create_model(init_mode='uniform', optimizer= 'adam', loss = 'mean_squared_error', drop_out= 0.5):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu, input_dim=111)) \n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(64, kernel_initializer=init_mode, activation=tf.nn.relu))\n",
    "    model.add(Dense(1, kernel_initializer=init_mode,activation='linear'))\n",
    "    # compile model\n",
    "    model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# %%time\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "batch_size = [ 20 ]\n",
    "epochs = [50]\n",
    "drop_out = [0.1]\n",
    "\n",
    "model_CV = KerasRegressor(build_fn=create_model, verbose=1,)\n",
    "# define the grid search parameters\n",
    "init_mode = ['normal']\n",
    "optimizer = [ 'Nadam']\n",
    "loss = [ 'mean_squared_logarithmic_error']\n",
    "\n",
    "\n",
    "param_grid = dict(init_mode=init_mode,optimizer=optimizer , loss = loss,drop_out=drop_out,epochs=epochs, \n",
    "                           batch_size=batch_size)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train, validation_split = 0.2, callbacks=[early_stop])\n",
    "\n",
    "\n",
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 20,\n",
       " 'drop_out': 0.1,\n",
       " 'epochs': 50,\n",
       " 'init_mode': 'normal',\n",
       " 'loss': 'mean_squared_logarithmic_error',\n",
       " 'optimizer': 'Nadam'}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_result.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292,)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sc_y.inverse_transform(y_pred.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8145275021715095"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0012 - mse: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0011514388024806976"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_['mean_test_score'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0127 - mse: 0.0193 - val_loss: 0.0062 - val_mse: 0.0100\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0074 - mse: 0.0122 - val_loss: 0.0061 - val_mse: 0.0098\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0072 - mse: 0.0119 - val_loss: 0.0059 - val_mse: 0.0095\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0070 - mse: 0.0117 - val_loss: 0.0058 - val_mse: 0.0095\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0069 - mse: 0.0115 - val_loss: 0.0056 - val_mse: 0.0091\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0067 - mse: 0.0111 - val_loss: 0.0055 - val_mse: 0.0090\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0107 - val_loss: 0.0052 - val_mse: 0.0085\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0061 - mse: 0.0103 - val_loss: 0.0049 - val_mse: 0.0080\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0058 - mse: 0.0098 - val_loss: 0.0047 - val_mse: 0.0076\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0053 - mse: 0.0090 - val_loss: 0.0042 - val_mse: 0.0070\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0048 - mse: 0.0084 - val_loss: 0.0038 - val_mse: 0.0064\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0045 - mse: 0.0079 - val_loss: 0.0036 - val_mse: 0.0060\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0071 - val_loss: 0.0034 - val_mse: 0.0056\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0040 - mse: 0.0070 - val_loss: 0.0032 - val_mse: 0.0054\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0037 - mse: 0.0064 - val_loss: 0.0030 - val_mse: 0.0052\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0035 - mse: 0.0062 - val_loss: 0.0029 - val_mse: 0.0050\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0034 - mse: 0.0061 - val_loss: 0.0029 - val_mse: 0.0049\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0033 - mse: 0.0058 - val_loss: 0.0027 - val_mse: 0.0046\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.0032 - mse: 0.0057 - val_loss: 0.0027 - val_mse: 0.0046\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0031 - mse: 0.0055 - val_loss: 0.0026 - val_mse: 0.0043\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0030 - mse: 0.0054 - val_loss: 0.0025 - val_mse: 0.0042\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0030 - mse: 0.0054 - val_loss: 0.0024 - val_mse: 0.0041\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0029 - mse: 0.0051 - val_loss: 0.0023 - val_mse: 0.0040\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0027 - mse: 0.0048 - val_loss: 0.0023 - val_mse: 0.0039\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0027 - mse: 0.0048 - val_loss: 0.0022 - val_mse: 0.0038\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0027 - mse: 0.0048 - val_loss: 0.0021 - val_mse: 0.0036\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0024 - mse: 0.0042 - val_loss: 0.0021 - val_mse: 0.0035\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0024 - mse: 0.0042 - val_loss: 0.0020 - val_mse: 0.0035\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0022 - mse: 0.0039 - val_loss: 0.0020 - val_mse: 0.0034\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0023 - mse: 0.0040 - val_loss: 0.0020 - val_mse: 0.0033\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0023 - mse: 0.0040 - val_loss: 0.0019 - val_mse: 0.0032\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0022 - mse: 0.0039 - val_loss: 0.0019 - val_mse: 0.0031\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0022 - mse: 0.0038 - val_loss: 0.0019 - val_mse: 0.0031\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0021 - mse: 0.0038 - val_loss: 0.0018 - val_mse: 0.0031\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0020 - mse: 0.0036 - val_loss: 0.0018 - val_mse: 0.0030\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0021 - mse: 0.0037 - val_loss: 0.0018 - val_mse: 0.0030\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0020 - mse: 0.0036 - val_loss: 0.0017 - val_mse: 0.0029\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0020 - mse: 0.0035 - val_loss: 0.0017 - val_mse: 0.0029\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0019 - mse: 0.0034 - val_loss: 0.0017 - val_mse: 0.0028\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0020 - mse: 0.0034 - val_loss: 0.0017 - val_mse: 0.0028\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0020 - mse: 0.0035 - val_loss: 0.0017 - val_mse: 0.0028\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0020 - mse: 0.0036 - val_loss: 0.0016 - val_mse: 0.0027\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0020 - mse: 0.0036 - val_loss: 0.0016 - val_mse: 0.0027\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0019 - mse: 0.0035 - val_loss: 0.0016 - val_mse: 0.0027\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0019 - mse: 0.0034 - val_loss: 0.0016 - val_mse: 0.0027\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0018 - mse: 0.0032 - val_loss: 0.0016 - val_mse: 0.0026\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0018 - mse: 0.0032 - val_loss: 0.0016 - val_mse: 0.0026\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0018 - mse: 0.0031 - val_loss: 0.0016 - val_mse: 0.0026\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0018 - mse: 0.0032 - val_loss: 0.0016 - val_mse: 0.0026\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.0018 - mse: 0.0032 - val_loss: 0.0015 - val_mse: 0.0025\n",
      "Best Accuracy for -0.002079074348633488 using {'neurons': 25, 'momentum': 0.9, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'relu'}\n",
      " mean=-0.03022, std=0.01592 using {'neurons': 25, 'momentum': 0.2, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'softmax'}\n",
      " mean=-0.007319, std=0.0006554 using {'neurons': 25, 'momentum': 0.9, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'softmax'}\n",
      " mean=-0.002079, std=0.0001351 using {'neurons': 25, 'momentum': 0.9, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'relu'}\n",
      " mean=-0.04102, std=0.001366 using {'neurons': 25, 'momentum': 0.2, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'hard_sigmoid'}\n",
      " mean=-0.01804, std=0.01566 using {'neurons': 25, 'momentum': 0.4, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'softmax'}\n",
      " mean=-0.003234, std=0.0001508 using {'neurons': 25, 'momentum': 0.6, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'linear'}\n",
      " mean=-0.005252, std=0.0007372 using {'neurons': 25, 'momentum': 0.6, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'relu'}\n",
      " mean=-0.04102, std=0.001366 using {'neurons': 25, 'momentum': 0.8, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'hard_sigmoid'}\n",
      " mean=-0.01841, std=0.01523 using {'neurons': 25, 'momentum': 0.0, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'hard_sigmoid'}\n",
      " mean=-0.03022, std=0.01593 using {'neurons': 25, 'momentum': 0.0, 'loss': 'mean_squared_logarithmic_error', 'learn_rate': 0.01, 'init_mode': 'normal', 'epochs': 50, 'drop_out': 0.1, 'batch_size': 20, 'activation': 'softmax'}\n"
     ]
    }
   ],
   "source": [
    "# let's create a function that creates the model (required for KerasClassifier) \n",
    "# while accepting the hyperparameters we want to tune \n",
    "\n",
    "\n",
    "early_stop = EarlyStopping( monitor='val_loss',verbose=1, patience=20)\n",
    "\n",
    "def create_model(momentum=0.0,activation='relu',neurons= 25, init_mode='uniform', learn_rate= 0.01, loss = 'mean_squared_error', drop_out= 0.5):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, kernel_initializer=init_mode, activation=activation, input_dim=111)) \n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(neurons, kernel_initializer=init_mode, activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer=init_mode,activation='linear'))\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=learn_rate, momentum= momentum)\n",
    "    model.compile(loss=loss,\n",
    "              optimizer=opt,\n",
    "              metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# %%time\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "batch_size = [ 20 ]\n",
    "epochs = [50]\n",
    "drop_out = [0.1]\n",
    "# patience = [10,20]\n",
    "\n",
    "model_CV = KerasRegressor(build_fn=create_model, verbose=1,)\n",
    "# define the grid search parameters\n",
    "init_mode = ['normal']\n",
    "optimizer = [ 'Nadam']\n",
    "loss = [ 'mean_squared_logarithmic_error']\n",
    "learn_rate = [0.01]\n",
    "neurons = [ 25]\n",
    "# activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "activation = ['softmax', 'relu','hard_sigmoid', 'linear']\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "\n",
    "\n",
    "param_grid = dict(init_mode=init_mode,learn_rate=learn_rate ,loss = loss,drop_out=drop_out,epochs=epochs, \n",
    "                           batch_size=batch_size,neurons=neurons, activation=activation,momentum=momentum )\n",
    "\n",
    "\n",
    "# grid = RandomizedSearchCV(estimator=model_CV, param_distributions=param_grid, n_jobs=-1, cv=3)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train, validation_split = 0.2, callbacks=[early_stop])\n",
    "\n",
    "\n",
    "# print results\n",
    "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neurons': 25,\n",
       " 'momentum': 0.9,\n",
       " 'loss': 'mean_squared_logarithmic_error',\n",
       " 'learn_rate': 0.01,\n",
       " 'init_mode': 'normal',\n",
       " 'epochs': 50,\n",
       " 'drop_out': 0.1,\n",
       " 'batch_size': 20,\n",
       " 'activation': 'relu'}"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_result.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622996100593953"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_['mean_test_score'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
